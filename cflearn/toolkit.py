import io
import os
import sys
import copy
import json
import math
import torch
import random
import hashlib
import argparse
import urllib.request

import numpy as np
import torch.nn as nn
import torch.nn.functional as F

from PIL import Image
from PIL import ImageDraw
from enum import Enum
from torch import Tensor
from typing import Any
from typing import Set
from typing import Dict
from typing import List
from typing import Tuple
from typing import Union
from typing import TypeVar
from typing import Callable
from typing import Iterable
from typing import Optional
from typing import NamedTuple
from typing import ContextManager
from pathlib import Path
from zipfile import ZipFile
from collections import defaultdict
from collections import OrderedDict
from torch.optim import Optimizer
from cftool.misc import prod
from cftool.misc import print_info
from cftool.misc import print_warning
from cftool.misc import check_requires
from cftool.misc import shallow_copy_dict
from cftool.misc import truncate_string_to_length
from cftool.misc import DownloadProgressBar
from cftool.array import to_torch
from cftool.array import is_string
from cftool.array import to_standard
from cftool.types import arr_type
from cftool.types import np_dict_type
from cftool.types import tensor_dict_type
from safetensors.torch import load_file

from .schema import TPath
from .schema import data_type
from .schema import d_inp_type
from .schema import param_type
from .schema import device_type
from .constants import INPUT_KEY
from .constants import WORKSPACE_ENVIRON_KEY
from .parameters import OPT

try:
    import matplotlib.pyplot as plt
    from matplotlib.pyplot import figure as Figure
except:
    plt = Figure = None
try:
    from onnxruntime import InferenceSession
except:
    InferenceSession = None
try:
    import cv2
except:
    cv2 = None


# general


min_seed_value = np.iinfo(np.uint32).min
max_seed_value = np.iinfo(np.uint32).max


def new_seed() -> int:
    """
    Generates a new random seed.

    Returns
    -------
    int
        A new random seed.

    Examples
    --------
    >>> seed = new_seed()
    >>> print(seed)
    42

    """

    return random.randint(min_seed_value, max_seed_value)


def seed_everything(seed: int) -> int:
    """
    Seeds all random number generators.

    Parameters
    ----------
    seed : int
        The seed value to use.

    Returns
    -------
    int
        The seed value used.

    Notes
    -----
    This function seeds the random number generators for NumPy, Python's built-in random module,
    and PyTorch. It ensures reproducibility of random number generation across different runs.

    If the provided seed is not within the valid range, a new random seed will be generated within
    the valid range and used instead. A warning will be printed to notify the user.

    Examples
    --------
    >>> seed = 42
    >>> seed_everything(seed)
    42

    """

    if not min_seed_value <= seed <= max_seed_value:
        seed, old_seed = new_seed(), seed
        print_warning(
            f"{old_seed} is not in bounds, numpy accepts from {min_seed_value} to "
            f"{max_seed_value}, will use {seed} instead."
        )

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    return seed


def _get_environ_workspace() -> Optional[str]:
    """
    Get the workspace from the environment variable.
    This is used internally to set the default workspace.

    Returns
    -------
    Optional[str]
        The workspace path if available, None otherwise.

    """

    return os.environ.get(WORKSPACE_ENVIRON_KEY)


def _set_environ_workspace(workspace: str) -> None:
    """
    Set the (default) workspace in the environment variable.

    Parameters
    ----------
    workspace : str
        The workspace path.

    """

    os.environ[WORKSPACE_ENVIRON_KEY] = workspace


def check_is_ci() -> bool:
    """
    Check if the code is running in a continuous integration (CI) environment.

    Returns
    -------
    bool
        True if running in a CI environment, False otherwise.

    Examples
    --------
    >>> is_ci = check_is_ci()
    >>> print(is_ci)
    False

    """

    parser = argparse.ArgumentParser()
    parser.add_argument("--ci", type=int, default=0)
    args = parser.parse_args()
    return bool(args.ci)


class FileInfo(NamedTuple):
    """
    Represents information about a (remote) file, often generated by
    `check_available` / `_get_file_info`.

    Attributes
    ----------
    sha : str
        The sha of the (remote) file.
    st_size : int
        The size of the (remote) file in bytes.
    download_url : Optional[str]
        The download url of the (remote) file, if available.

    """

    sha: str
    st_size: int
    download_url: Optional[str] = None


def check_available(dtype: str, tag: str) -> Optional[FileInfo]:
    """
    Check if a specific data type and tag are available in the zoo.

    Parameters
    ----------
    dtype : str (DownloadDtype)
        The data type to check for, it should be the key of the dictionary in `available.json`.
        > Specifically, it should actually be one of the values in `DownloadDtype`.
    tag : str
        The tag to check for, it should be the sub-key of the dictionary in `available.json`.

    Returns
    -------
    Optional[FileInfo]
        Returns a FileInfo object if the dtype and tag are available, otherwise returns None.

    Examples
    --------
    >>> check_available("checkpoints", "ldm.sd")
    FileInfo(
        sha='dbb178bf92a5e57be6d82b2627189790577f4b85675e93281828d9fc35a263a2',
        st_size=2134032457,
        download_url='https://huggingface.co/carefree0910/carefree-learn/resolve/main/checkpoints_v0.4.x/ldm.sd.pt'
    )

    """

    available_path = Path(__file__).parent / "zoo" / "available.json"
    with available_path.open("r") as f:
        available = json.load(f)
    info = available[dtype].get(tag)
    return None if info is None else FileInfo(**info)


def get_file_size(path: Path) -> int:
    """
    Get the size of a file.

    Parameters
    ----------
    path : Path
        The path of the file.

    Returns
    -------
    int
        The size of the file in bytes.

    Examples
    --------
    >>> get_file_size(Path("..."))

    """

    return path.stat().st_size


def get_file_info(path: Path) -> FileInfo:
    """
    Get the information of a file.

    Parameters
    ----------
    path : Path
        The path of the file.

    Returns
    -------
    FileInfo
        The FileInfo object containing information about the file.

    Examples
    --------
    >>> get_file_info(Path("..."))

    """

    with path.open("rb") as f:
        sha = hashlib.sha256(f.read()).hexdigest()
    return FileInfo(sha, get_file_size(path))


def check_sha_with(path: Path, tgt_sha: str) -> bool:
    """
    Check if the SHA256 hash of a file matches a target hash.

    Parameters
    ----------
    path : Path
        The path of the file.
    tgt_sha : str
        The target SHA256 hash to compare with.

    Returns
    -------
    bool
        True if the file's hash matches the target hash, False otherwise.

    Examples
    --------
    >>> check_sha_with(Path("..."), "...")

    """

    return get_file_info(path).sha == tgt_sha


class DownloadDtype(str, Enum):
    TOKENIZERS = "tokenizers"
    CHECKPOINTS = "checkpoints"
    REFERENCES = "references"
    DATASETS = "datasets"
    JSONS = "jsons"


download_extensions = {
    DownloadDtype.TOKENIZERS: ".pkl",
    DownloadDtype.CHECKPOINTS: ".pt",
    DownloadDtype.REFERENCES: ".pt",
    DownloadDtype.DATASETS: ".zip",
    DownloadDtype.JSONS: ".json",
}


def get_download_root(dtype: DownloadDtype) -> Path:
    """
    Get the root directory for downloads of a specific type.

    Parameters
    ----------
    dtype : DownloadDtype
        The type of download.

    Returns
    -------
    Path
        The root directory for downloads of the specified type.

    Examples
    --------
    >>> get_download_root(DownloadDtype.TOKENIZERS)

    """

    return OPT.cache_dir / dtype.value


class DownloadPathInfo(NamedTuple):
    """
    Represents the information of a download path.

    Attributes
    ----------
    info : FileInfo
        The FileInfo object containing information about the file.
    download_root : Path
        The root directory for downloads.
    download_path : Path
        The path of the download.

    """

    info: FileInfo
    download_root: Path
    download_path: Path


def get_download_path_info(
    dtype: DownloadDtype,
    tag: str,
    download_root: Optional[TPath] = None,
    *,
    extension: Optional[str] = None,
) -> DownloadPathInfo:
    """
    Get the download path information for a specific download.

    Parameters
    ----------
    dtype : DownloadDtype
        The type of download.
    tag : str
        The tag of the download.
    download_root : Optional[TPath], optional
        The root directory for downloads (default is None).
    extension : Optional[str], optional
        The file extension for the download (default is None).

    Returns
    -------
    DownloadPathInfo
        The download path information.

    Raises
    ------
    ValueError
        If the download is not available or if the extension is not defined.

    Examples
    --------
    >>> get_download_path_info(DownloadDtype.CHECKPOINTS, "ldm.sd")
    DownloadPathInfo(
        info=FileInfo(
            sha="dbb178bf92a5e57be6d82b2627189790577f4b85675e93281828d9fc35a263a2",
            st_size=2134032457,
            download_url="https://huggingface.co/carefree0910/carefree-learn/resolve/main/checkpoints_v0.4.x/ldm.sd.pt",
        ),
        download_root=Path("~/.cache/carefree-learn/checkpoints"),
        download_path=Path("~/.cache/carefree-learn/checkpoints/ldm.sd.pt"),
    )

    """

    info = check_available(dtype, tag)
    if info is None:
        raise ValueError(f"'{tag}' is currently not available at '{dtype}'")
    if download_root is None:
        download_root = get_download_root(dtype)
    if isinstance(download_root, str):
        download_root = Path(download_root)
    download_root.mkdir(exist_ok=True, parents=True)
    if extension is None:
        extension = download_extensions.get(dtype)
    if extension is None:
        raise ValueError(f"extension is not defined for '{dtype}'")
    file = f"{tag}{extension}"
    download_path = download_root / file
    return DownloadPathInfo(info, download_root, download_path)


def download(
    dtype: DownloadDtype,
    tag: str,
    download_root: Optional[TPath] = None,
    *,
    extension: Optional[str] = None,
    check_sha: bool = False,
    remove_zip: bool = True,
) -> Path:
    """
    Download a file of a specific type and tag.

    Parameters
    ----------
    dtype : DownloadDtype
        The type of download.
    tag : str
        The tag of the file to download.
    download_root : Optional[TPath]
        The root directory for downloads. If None, the default root directory for the specified type is used.
    extension : Optional[str]
        The extension of the file to download. If None, the default extension for the specified type is used.
    check_sha : bool
        Whether to check the SHA256 hash of the downloaded file.
    remove_zip : bool
        Whether to remove the downloaded zip file after extraction.

    Returns
    -------
    Path
        The path of the downloaded file.

    Examples
    --------
    >>> download(DownloadDtype.CHECKPOINTS, "ldm.sd")
    Path('~/.cache/carefree-learn/checkpoints/ldm.sd.pt')

    """

    info, download_root, download_path = get_download_path_info(
        dtype, tag, download_root, extension=extension
    )
    is_zip = extension == ".zip"
    zip_download_folder = download_root / tag
    if is_zip and zip_download_folder.is_dir():
        return zip_download_folder
    fmt = "cache file is detected but {}, it will be re-downloaded"
    if not is_zip and download_path.is_file():
        if get_file_size(download_path) != info.st_size:
            print_warning(fmt.format("st_size is not correct"))
        else:
            if not check_sha or check_sha_with(download_path, info.sha):
                return download_path
            print_warning(fmt.format("sha is not correct"))
    with DownloadProgressBar(unit="B", unit_scale=True, miniters=1, desc=tag) as t:
        if info.download_url is not None:
            url = info.download_url
        else:
            prefix = f"https://github.com/carefree0910/carefeee-learn-assets/releases/download/{dtype}/"
            url = f"{prefix}{download_path.name}"
        urllib.request.urlretrieve(
            url,
            filename=download_path,
            reporthook=t.update_to,
        )
    if not is_zip:
        return download_path
    with ZipFile(download_path, "r") as zip_ref:
        zip_ref.extractall(zip_download_folder)
    if remove_zip:
        os.remove(download_path)
    return zip_download_folder


def download_tokenizer(
    tag: str,
    download_root: Optional[TPath] = None,
    *,
    extension: Optional[str] = None,
    check_sha: bool = False,
    remove_zip: bool = True,
) -> Path:
    """
    Download a tokenizer.

    Parameters
    ----------
    tag : str
        The tag of the tokenizer to download.
    download_root : Optional[TPath]
        The root directory for downloads. If None, the default root directory for tokenizers is used.
    extension : Optional[str]
        The extension of the tokenizer to download. If None, the default extension for tokenizers is used.
    check_sha : bool
        Whether to check the SHA256 hash of the downloaded tokenizer.
    remove_zip : bool
        Whether to remove the downloaded zip file after extraction.

    Returns
    -------
    Path
        The path of the downloaded tokenizer.

    Examples
    --------
    >>> download_tokenizer("...")

    """

    return download(
        DownloadDtype.TOKENIZERS,
        tag,
        download_root,
        extension=extension,
        check_sha=check_sha,
        remove_zip=remove_zip,
    )


def download_checkpoint(
    tag: str,
    download_root: Optional[TPath] = None,
    *,
    extension: Optional[str] = None,
    check_sha: bool = False,
    remove_zip: bool = True,
) -> Path:
    """
    Download a checkpoint.

    Parameters
    ----------
    tag : str
        The tag of the checkpoint to download.
    download_root : Optional[TPath]
        The root directory for downloads. If None, the default root directory for checkpoints is used.
    extension : Optional[str]
        The extension of the checkpoint to download. If None, the default extension for checkpoints is used.
    check_sha : bool
        Whether to check the SHA256 hash of the downloaded checkpoint.
    remove_zip : bool
        Whether to remove the downloaded zip file after extraction.

    Returns
    -------
    Path
        The path of the downloaded checkpoint.

    Examples
    --------
    >>> download_checkpoint("...")

    """

    return download(
        DownloadDtype.CHECKPOINTS,
        tag,
        download_root,
        extension=extension,
        check_sha=check_sha,
        remove_zip=remove_zip,
    )


def download_json(
    tag: str,
    download_root: Optional[TPath] = None,
    *,
    extension: Optional[str] = None,
    check_sha: bool = False,
    remove_zip: bool = True,
) -> Path:
    """
    Download a json.

    Parameters
    ----------
    tag : str
        The tag of the json to download.
    download_root : Optional[TPath]
        The root directory for downloads. If None, the default root directory for jsons is used.
    extension : Optional[str]
        The extension of the json to download. If None, the default extension for jsons is used.
    check_sha : bool
        Whether to check the SHA256 hash of the downloaded json.
    remove_zip : bool
        Whether to remove the downloaded zip file after extraction.

    Returns
    -------
    Path
        The path of the downloaded json.

    Examples
    --------
    >>> download_json("...")

    """

    return download(
        DownloadDtype.JSONS,
        tag,
        download_root,
        extension=extension,
        check_sha=check_sha,
        remove_zip=remove_zip,
    )


def get_compatible_name(
    dtype: DownloadDtype,
    name: str,
    versions: List[Tuple[int, int]],
    *,
    bc: bool = False,
) -> str:
    """
    Get a compatible name for a download.

    Parameters
    ----------
    dtype : DownloadDtype
        The type of download.
    name : str
        The original name of the download.
    versions : List[Tuple[int, int]]
        A list of compatible versions.
    bc : bool
        Whether to ensure backward compatibility.

    Returns
    -------
    str
        The compatible name for the download.

    Examples
    --------
    >>> get_compatible_name(DownloadDtype.TOKENIZERS, "clip", [(3, 8), (3, 9)])
    'clip_3.9'

    """

    version_info = sys.version_info
    version = None
    if bc:
        tgt_versions = list(
            filter(
                lambda ver: version_info.major < ver[0] or version_info.minor < ver[1],
                versions,
            )
        )
        if tgt_versions is not None:
            version = max(tgt_versions)
    if not bc:
        tgt_versions = list(
            filter(
                lambda ver: version_info.major > ver[0] or version_info.minor >= ver[1],
                versions,
            )
        )
        if tgt_versions is not None:
            version = max(tgt_versions)
    if version is not None:
        compatible_name = f"{name}_{version[0]}.{version[1]}"
        if check_available(dtype, compatible_name):
            name = compatible_name
        else:
            print_warning(
                f"compatible name '{compatible_name}' is not available "
                f"on the server, will use the original name ({name}) instead"
            )
    return name


def show_or_save(
    export_path: Optional[str],
    fig: Optional[Figure] = None,
    **kwargs: Any,
) -> None:
    """
    Utility function to deal with figure.

    Parameters
    ----------
    export_path : {None, str}
    * If None, the figure will be shown.
    * If str, it represents the path where the figure should be saved to.
    fig : {None, plt.Figure}
    * If None, default figure contained in plt will be executed.
    * If plt.figure, it will be executed

    """

    if plt is None:
        raise ValueError("`matplotlib` is needed for `show_or_save`")
    if export_path is None:
        fig.show(**kwargs) if fig is not None else plt.show(**kwargs)
    else:
        if fig is not None:
            fig.savefig(export_path)
        else:
            plt.savefig(export_path, **kwargs)
    plt.close()


def show_or_return(return_canvas: bool) -> Union[None, np.ndarray]:
    """
    Utility function to deal with current plt.

    Parameters
    ----------
    return_canvas : bool, whether return canvas or not.

    """

    if plt is None:
        raise ValueError("`matplotlib` is needed for `show_or_return`")
    if not return_canvas:
        plt.show()
        return None

    buffer_ = io.BytesIO()
    plt.savefig(buffer_, format="png")
    plt.close()
    buffer_.seek(0)
    image = Image.open(buffer_)
    canvas = np.asarray(image)[..., :3]
    buffer_.close()
    return canvas


class WeightsStrategy:
    """
    A strategy for generating sample weights.

    Parameters
    ----------
    strategy : Optional[str]
        The name of the strategy to use. Should be one of "linear_decay", "radius_decay",
        "log_decay", "sigmoid_decay".
        If None, no weights are generated.

    Examples
    --------
    >>> ws = WeightsStrategy("linear_decay")
    >>> ws(10)
    array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])

    """

    def __init__(self, strategy: Optional[str]):
        self.strategy = strategy

    def __call__(self, num: int) -> Optional[np.ndarray]:
        """
        Generate sample weights.

        Parameters
        ----------
        num : int
            The number of data samples.

        Returns
        -------
        np.ndarray
            The generated sample weights.
        """

        if self.strategy is None:
            return None
        return getattr(self, self.strategy)(num)

    def linear_decay(self, num: int) -> np.ndarray:
        """
        Generate sample weights using a linear decay strategy.

        Parameters
        ----------
        num : int
            The number of data samples.

        Returns
        -------
        np.ndarray
            The generated sample weights.
        """

        return np.linspace(0, 1, num + 1)[1:]

    def radius_decay(self, num: int) -> np.ndarray:
        """
        Generate sample weights using a radius decay strategy.

        Parameters
        ----------
        num : int
            The number of data samples.

        Returns
        -------
        np.ndarray
            The generated sample weights.
        """

        return np.sin(np.arccos(1.0 - np.linspace(0, 1, num + 1)[1:]))

    def log_decay(self, num: int) -> np.ndarray:
        """
        Generate sample weights using a log decay strategy.

        Parameters
        ----------
        num : int
            The number of data samples.

        Returns
        -------
        np.ndarray
            The generated sample weights.
        """

        return np.log(np.arange(num) + np.e)

    def sigmoid_decay(self, num: int) -> np.ndarray:
        """
        Generate sample weights using a sigmoid decay strategy.

        Parameters
        ----------
        num : int
            The number of data samples.

        Returns
        -------
        np.ndarray
            The generated sample weights.
        """

        x = np.linspace(-5.0, 5.0, num)
        return 1.0 / (1.0 + np.exp(-x))

    def visualize(self, export_path: str = "weights_strategy.png") -> None:
        """
        Visualize the weights strategy.

        Parameters
        ----------
        export_path : str
            The path to save the visualization. If None, the visualization is shown instead of being saved.

        Examples
        --------
        >>> ws = WeightsStrategy("linear_decay")
        >>> ws.visualize("weights_strategy.png")

        """

        if plt is None:
            raise ValueError("`matplotlib` is needed for `visualize`")
        if show_or_save is None:
            raise ValueError("`carefree-ml` is needed for `visualize`")
        n = 1000
        x = np.linspace(0, 1, n)
        y = self(n)
        if isinstance(y, tuple):
            y = y[0]
        plt.figure()
        plt.plot(x, y)
        show_or_save(export_path)


# dl


pt2_sdp_attn = getattr(F, "scaled_dot_product_attention", None)
warnings = set()
xformers_failed = set()
GenericM = TypeVar("GenericM", bound=nn.Module)


def warn_once(message: str, *, key: Optional[str] = None) -> None:
    """
    Print a warning message once.

    Parameters
    ----------
    message : str
        The warning message to print.
    key : Optional[str]
        The key associated with the warning. If None, the message is used as the key.

    Examples
    --------
    >>> warn_once("This is a warning message")
    > [warning] This is a warning message
    >>> warn_once("This is a warning message")
    # nothing is printed

    """

    key = key or message
    if key not in warnings:
        print_warning(message)
        warnings.add(key)


def try_run_xformers_sdp_attn(
    q: Tensor,
    k: Tensor,
    v: Tensor,
    training: bool,
    mask: Optional[Tensor] = None,
    p: Optional[float] = None,
) -> Optional[Tensor]:
    """
    Try to run the xformers sdp attention function.

    Parameters
    ----------
    q : Tensor
        The query tensor.
    k : Tensor
        The key tensor.
    v : Tensor
        The value tensor.
    training : bool
        Whether the model is in training mode.
    mask : Optional[Tensor]
        The mask tensor. If None, no mask is applied.
    p : Optional[float]
        The dropout probability. If None, no dropout is applied.

    Returns
    -------
    Optional[Tensor]
        The output tensor if the xformers sdp attention function runs successfully, otherwise None.

    Examples
    --------
    >>> q = torch.randn(3, 32, 64).to("cuda")
    >>> k = torch.randn(3, 32, 64).to("cuda")
    >>> v = torch.randn(3, 32, 64).to("cuda")
    >>> try_run_xformers_sdp_attn(q, k, v, True)
    tensor([...])

    """

    message = f"\nq: {q.dtype}, {q.shape}; k: {k.dtype}, {k.shape}; v: {v.dtype}, {v.shape}; mask: {None if mask is None else f'{mask.dtype}, {mask.shape}'}; p: {p}; training: {training}\n"
    if message in xformers_failed:
        return None

    try:
        import xformers.ops

        if p is None:
            p = 0.0
        transpose = lambda t: t if len(t.shape) == 3 else t.transpose(1, 2)
        q, k, v = map(transpose, (q, k, v))
        if mask is not None:
            if torch.allclose(mask, ~torch.triu(torch.ones_like(mask), diagonal=1)):
                from xformers.ops.fmha.attn_bias import LowerTriangularMask

                mask = LowerTriangularMask()
            else:
                mask = torch.where(mask, 0.0, float("-inf"))
        ret = xformers.ops.memory_efficient_attention(q, k, v, mask, p)
        if len(ret.shape) == 4:
            ret = ret.transpose(1, 2)
        return ret.contiguous()
    except Exception as err:
        warn_once(f"failed to run `xformers` sdp attn: {err}, details: {message}")
        xformers_failed.add(message)
        return None


def sdp_attn(
    q: Tensor,
    k: Tensor,
    v: Tensor,
    training: bool,
    mask: Optional[Tensor] = None,
    dropout: Optional[float] = None,
) -> Tensor:
    """
    Run sdp attention function. It will try to run the xformers sdp attention function
    first, and fall back to the native pytorch implementation if the xformers function fails.

    Parameters
    ----------
    q : Tensor
        The query tensor.
    k : Tensor
        The key tensor.
    v : Tensor
        The value tensor.
    training : bool
        Whether the model is in training mode.
    mask : Optional[Tensor]
        The mask tensor. If None, no mask is applied.
    dropout : Optional[float]
        The dropout probability. If None, no dropout is applied.

    Returns
    -------
    Tensor
        The result tensor.

    Examples
    --------
    >>> q = torch.randn(3, 32, 64).to("cuda")
    >>> k = torch.randn(3, 32, 64).to("cuda")
    >>> v = torch.randn(3, 32, 64).to("cuda")
    >>> sdp_attn(q, k, v, True)
    tensor([...])

    """

    q = q.contiguous()
    k = k.contiguous()
    v = v.contiguous()
    try_xformers = try_run_xformers_sdp_attn(q, k, v, training, mask, dropout)
    if try_xformers is not None:
        return try_xformers
    size = q.shape[0]
    if mask is not None and len(mask.shape) == 3:
        b = mask.shape[0]
        mask = mask.view(b, -1)
        mask = mask[:, None, :].repeat(size // b, 1, 1)
    if pt2_sdp_attn is not None:
        dropout = dropout if training else None
        dropout = 0.0 if dropout is None else dropout
        return pt2_sdp_attn(q, k, v, mask, dropout)
    warn_once(
        "failed to run `scaled_dot_product_attention` from pytorch 2.x, "
        "will use native `torch` implementations instead"
    )
    raw_weights = q @ k.transpose(-2, -1) / math.sqrt(k.shape[-1])
    if mask is not None:
        raw_weights.masked_fill_(~mask, float("-inf"))
    weights = F.softmax(raw_weights, dim=-1)
    if training and dropout is not None and 0.0 < dropout < 1.0:
        weights = F.dropout(weights, dropout)
    return weights @ v


def get_tensors(inp: d_inp_type) -> tensor_dict_type:
    """
    Get tensors from input.

    Parameters
    ----------
    inp : d_inp_type
        The input from which to get the tensors. This could be a path to a file or a dictionary.

    Returns
    -------
    tensor_dict_type
        A dictionary of tensors.

    Examples
    --------
    >>> get_tensors("example.safetensors")
    {'tensor1': tensor([...]), 'tensor2': tensor([...])}

    """

    if isinstance(inp, Path):
        inp = str(inp)
    if isinstance(inp, str):
        if inp.endswith(".safetensors"):
            inp = load_file(inp)
        else:
            inp = torch.load(inp, map_location="cpu")
    if "state_dict" in inp:
        inp = inp["state_dict"]
    return shallow_copy_dict(inp)


def get_dtype(m: nn.Module) -> torch.dtype:
    """
    Get the data type of the parameters of a module.

    Parameters
    ----------
    m : nn.Module
        The module.

    Returns
    -------
    torch.dtype
        The data type of the parameters of the module.

    Examples
    --------
    >>> m = nn.Linear(10, 2)
    >>> get_dtype(m)
    torch.float32

    """

    params = list(m.parameters())
    return torch.float32 if not params else params[0].dtype


def get_device(m: nn.Module) -> torch.device:
    """
    Get the device of the parameters of a module.

    Parameters
    ----------
    m : nn.Module
        The module.

    Returns
    -------
    torch.device
        The device of the parameters of the module.

    Examples
    --------
    >>> m = nn.Linear(10, 2)
    >>> get_device(m)
    device(type='cpu')

    """

    params = list(m.parameters())
    return torch.device("cpu") if not params else params[0].device


def get_clones(
    module: nn.Module,
    n: int,
    *,
    return_list: bool = False,
) -> Union[nn.ModuleList, List[nn.Module]]:
    """
    Get clones of a module.

    Parameters
    ----------
    module : nn.Module
        The module to clone.
    n : int
        The number of clones to create.
    return_list : bool, optional
        Whether to return the clones as a list. If False, the clones are returned as a ModuleList.

    Returns
    -------
    Union[nn.ModuleList, List[nn.Module]]
        The clones of the module.

    Examples
    --------
    >>> m = nn.Linear(10, 2)
    >>> get_clones(m, 3)
    ModuleList(
      (0-2): 3 x Linear(in_features=10, out_features=2, bias=True)
    )

    """

    module_list = [module]
    for _ in range(n - 1):
        module_list.append(copy.deepcopy(module))
    if return_list:
        return module_list
    return nn.ModuleList(module_list)


def get_torch_device(device: device_type) -> torch.device:
    """
    Get a torch device.

    Parameters
    ----------
    device : device_type
        The device to get. This could be None, an integer, a string, or a torch.device.

    Returns
    -------
    torch.device
        The torch device.

    Examples
    --------
    >>> get_torch_device("cuda:0")
    device(type='cuda', index=0)

    """

    if device is None:
        return torch.device("cpu")
    if isinstance(device, (int, str)):
        try:
            device = int(device)
        except:
            pass
        finally:
            device = torch.device(device)
    return device


def empty_cuda_cache(device: device_type) -> None:
    """
    Empty the CUDA cache on a specific device.

    Parameters
    ----------
    device : device_type
        The device on which to empty the CUDA cache.

    Examples
    --------
    >>> empty_cuda_cache("cuda:0")

    """

    device = get_torch_device(device)
    if device.type != "cuda":
        return
    with torch.cuda.device(device):
        torch.cuda.empty_cache()


def is_cpu(device: device_type) -> bool:
    """
    Check if a device is a CPU.

    Parameters
    ----------
    device : device_type
        The device to check.

    Returns
    -------
    bool
        True if the device is a CPU, False otherwise.

    Examples
    --------
    >>> is_cpu("cuda:0")
    False

    """

    return get_torch_device(device).type == "cpu"


def np_batch_to_tensor(np_batch: np_dict_type) -> tensor_dict_type:
    """
    Convert a batch of numpy arrays to tensors.

    Parameters
    ----------
    np_batch : np_dict_type
        The batch of numpy arrays to convert.

    Returns
    -------
    tensor_dict_type
        The batch of tensors.

    Examples
    --------
    >>> np_batch_to_tensor({"a": np.array([1, 2, 3]), "b": np.array([4, 5, 6])})
    {'a': tensor([1, 2, 3]), 'b': tensor([4, 5, 6])}

    """

    return {
        k: v if not isinstance(v, np.ndarray) or is_string(v) else to_torch(v)
        for k, v in np_batch.items()
    }


def tensor_batch_to_np(tensor_batch: np_dict_type) -> np_dict_type:
    """
    Convert a batch of tensors to numpy arrays.

    Parameters
    ----------
    tensor_batch : np_dict_type
        The batch of tensors to convert.

    Returns
    -------
    np_dict_type
        The batch of numpy arrays.

    Examples
    --------
    >>> tensor_batch_to_np({"a": torch.tensor([1, 2, 3]), "b": torch.tensor([4, 5, 6])})
    {'a': array([1, 2, 3], dtype=int64), 'b': array([4, 5, 6], dtype=int64)}

    """

    return {
        k: v if not isinstance(v, Tensor) else v.cpu().numpy()
        for k, v in tensor_batch.items()
    }


def safe_clip_(net: Tensor) -> None:
    """
    Clip the values of a tensor in-place to the valid range for its data type.

    Parameters
    ----------
    net : Tensor
        The tensor to clip.

    Examples
    --------
    >>> net = torch.tensor([1.0, 2.0, 3.0])
    >>> safe_clip_(net)
    >>> net
    tensor([1., 2., 3.])

    """

    finfo = torch.finfo(net.dtype)
    net.clamp_(finfo.min, finfo.max)


def insert_intermediate_dims(net: arr_type, ref: arr_type) -> arr_type:
    """
    Insert intermediate dimensions into a tensor or numpy array.

    Parameters
    ----------
    net : arr_type
        The tensor or numpy array to insert dimensions into, which should have 2 dimensions.
    ref : arr_type
        The reference tensor or numpy array. The output will have the
        same number of dimensions as this array.

    Returns
    -------
    arr_type
        The tensor or numpy array with inserted dimensions.

    Examples
    --------
    >>> net = torch.tensor([[1.0, 2.0, 3.0]])
    >>> ref = torch.tensor([[[1.0, 2.0, 3.0]]])
    >>> insert_intermediate_dims(net, ref)
    tensor([[[1., 2., 3.]]])

    """

    net_dim = len(net.shape)
    if net_dim != 2:
        raise ValueError(f"only 2-dim tensor is supported, but got {net_dim}")
    dim_diff = len(ref.shape) - net_dim
    if dim_diff == 0:
        return net
    new_shape = net.shape[0], *((1,) * dim_diff), net.shape[1]
    if isinstance(net, Tensor):
        return net.view(*new_shape)
    return net.reshape(new_shape)


def fix_denormal_states(
    states: tensor_dict_type,
    *,
    eps: float = 1.0e-32,
    verbose: bool = False,
) -> tensor_dict_type:
    """
    Fix denormal states in a dictionary of tensors.

    Parameters
    ----------
    states : tensor_dict_type
        The dictionary of tensors to fix.
    eps : float, optional
        The threshold below which a value is considered denormal. Default is 1.0e-32.
    verbose : bool, optional
        Whether to print information about the denormal ratio. Default is False.

    Returns
    -------
    tensor_dict_type
        The dictionary of tensors with denormal states fixed.

    Examples
    --------
    >>> states = {"a": torch.tensor([1.0, 2.0, 1.0e-33]), "b": torch.tensor([4.0, 5.0, 6.0])}
    >>> fix_denormal_states(states)
    {'a': tensor([1., 2., 0.]), 'b': tensor([4., 5., 6.])}

    """

    new_states = shallow_copy_dict(states)
    num_total = num_denormal_total = 0
    for k, v in states.items():
        if not v.is_floating_point():
            continue
        num_total += v.numel()
        denormal = (v != 0) & (v.abs() < eps)
        num_denormal = denormal.sum().item()
        num_denormal_total += num_denormal
        if num_denormal > 0:
            new_states[k][denormal] = v.new_zeros(num_denormal)
    if verbose:
        print_info(f"denormal ratio : {num_denormal_total / num_total:8.6f}")
    return new_states


def has_batch_norms(m: nn.Module) -> bool:
    """
    Check if a module has any batch normalization layers.

    Parameters
    ----------
    m : nn.Module
        The module to check.

    Returns
    -------
    bool
        True if the module has any batch normalization layers, False otherwise.

    Examples
    --------
    >>> m = nn.Sequential(nn.Linear(10, 2), nn.BatchNorm1d(2))
    >>> has_batch_norms(m)
    True

    """

    bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm)
    for name, module in m.named_modules():
        if isinstance(module, bn_types):
            return True
    return False


def inject_parameters(
    src: nn.Module,
    tgt: nn.Module,
    *,
    strict: Optional[bool] = None,
    src_filter_fn: Optional[Callable[[str], bool]] = None,
    tgt_filter_fn: Optional[Callable[[str], bool]] = None,
    custom_mappings: Optional[Dict[str, str]] = None,
    states_callback: Optional[Callable[[tensor_dict_type], tensor_dict_type]] = None,
) -> None:
    """
    Inject parameters from one module into another.

    Parameters
    ----------
    src : nn.Module
        The source module.
    tgt : nn.Module
        The target module.
    strict : Optional[bool], optional
        Whether to strictly enforce that the keys in the src and tgt state dicts match.
        Default is None, which means strict is True if tgt_filter_fn is None, and False otherwise.
    src_filter_fn : Optional[Callable[[str], bool]], optional
        A function that takes a key and returns True if the corresponding parameter should
        be included from the src state dict. Default is None, which means all parameters are included.
    tgt_filter_fn : Optional[Callable[[str], bool]], optional
        A function that takes a key and returns True if the corresponding parameter should
        be included from the tgt state dict. Default is None, which means all parameters are included.
    custom_mappings : Optional[Dict[str, str]], optional
        A dictionary mapping keys in the src state dict to keys in the tgt state dict.
        Default is None, which means the keys are assumed to match exactly.
    states_callback : Optional[Callable[[tensor_dict_type], tensor_dict_type]], optional
        A function that takes the src state dict and returns a new state dict.
        Default is None, which means the src state dict is used as is.

    Examples
    --------
    >>> src = nn.Linear(10, 2)
    >>> tgt = nn.Linear(10, 2)
    >>> inject_parameters(src, tgt)
    >>> assert torch.allclose(src.weight, tgt.weight)
    >>> assert torch.allclose(src.bias, tgt.bias)

    """

    if strict is None:
        strict = tgt_filter_fn is None
    src_states = src.state_dict()
    tgt_states = tgt.state_dict()
    if src_filter_fn is not None:
        pop_keys = [key for key in src_states if not src_filter_fn(key)]
        for key in pop_keys:
            src_states.pop(key)
    if tgt_filter_fn is not None:
        pop_keys = [key for key in tgt_states if not tgt_filter_fn(key)]
        for key in pop_keys:
            tgt_states.pop(key)
    if states_callback is not None:
        src_states = states_callback(shallow_copy_dict(src_states))
    if len(src_states) != len(tgt_states):
        raise ValueError(f"lengths of states are not identical between {src} and {tgt}")
    new_states = OrderedDict()
    if custom_mappings is not None:
        for src_k, tgt_k in custom_mappings.items():
            new_states[tgt_k] = src_states.pop(src_k)
            tgt_states.pop(tgt_k)
    for (src_k, src_v), (tgt_k, tgt_v) in zip(src_states.items(), tgt_states.items()):
        if src_v.shape != tgt_v.shape:
            raise ValueError(
                f"shape of {src_k} ({list(src_v.shape)}) is not identical with "
                f"shape of {tgt_k} ({list(tgt_v.shape)})"
            )
        new_states[tgt_k] = src_v
    tgt.load_state_dict(new_states, strict=strict)


class Diffs(NamedTuple):
    """
    A named tuple for storing the differences between parameters of two modules.

    Attributes
    ----------
    names1 : List[str]
        The names of the parameters in the first module.
    names2 : List[str]
        The names of the parameters in the second module.
    diffs : List[Tensor]
        The differences between the parameters of the two modules.

    """

    names1: List[str]
    names2: List[str]
    diffs: List[Tensor]


def sorted_param_diffs(m1: nn.Module, m2: nn.Module) -> Diffs:
    """
    Compute the sorted differences between the parameters of two modules,
    often used to check if two modules are identical.

    Parameters
    ----------
    m1 : nn.Module
        The first module.
    m2 : nn.Module
        The second module.

    Returns
    -------
    Diffs
        The differences between the parameters of the two modules.

    Raises
    ------
    ValueError
        If the lengths of the parameters of the two modules are not identical.

    Examples
    --------
    >>> m1 = nn.Linear(10, 2)
    >>> m2 = nn.Linear(10, 2)
    >>> sorted_param_diffs(m1, m2)
    Diffs(names1=['weight', 'bias'], names2=['weight', 'bias'], diffs=[tensor([...]), tensor([...])])

    """

    names1, params1 = zip(*m1.named_parameters())
    names2, params2 = zip(*m2.named_parameters())
    if len(params1) != len(params2):
        raise ValueError(f"lengths of params are not identical between {m1} and {m2}")
    diffs = []
    for p1, p2 in zip(params1, params2):
        (p1, _), (p2, _) = map(torch.sort, [p1.view(-1), p2.view(-1)])
        diffs.append(torch.abs(p1.data - p2.data))
    return Diffs(list(names1), list(names2), diffs)


def get_gradient(
    y: Tensor,
    x: Tensor,
    retain_graph: bool = False,
    create_graph: bool = False,
) -> Union[Tensor, Tuple[Tensor, ...]]:
    """
    Compute the gradient of y with respect to x.

    Parameters
    ----------
    y : Tensor
        The tensor to compute the gradient of.
    x : Tensor
        The tensor to compute the gradient with respect to.
    retain_graph : bool, optional
        Whether to retain the computation graph. Default is False.
    create_graph : bool, optional
        Whether to create a computation graph. Default is False.

    Returns
    -------
    Union[Tensor, Tuple[Tensor, ...]]
        The gradient of y with respect to x.

    Examples
    --------
    >>> x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
    >>> y = x ** 2
    >>> get_gradient(y, x)
    tensor([2., 4., 6.])

    """

    grads = torch.autograd.grad(y, x, torch.ones_like(y), retain_graph, create_graph)
    return grads[0] if len(grads) == 1 else grads


def set_requires_grad(module: nn.Module, requires_grad: bool = False) -> None:
    """
    Set the `requires_grad` attribute of all parameters of a module.

    Parameters
    ----------
    module : nn.Module
        The module.
    requires_grad : bool, optional
        The value to set the `requires_grad` attribute to. Default is False.

    Examples
    --------
    >>> m = nn.Linear(10, 2)
    >>> set_requires_grad(m, True)

    """

    for param in module.parameters():
        param.requires_grad = requires_grad


def to_eval(module: GenericM) -> GenericM:
    """
    Set a module to evaluation mode and disable gradient computation for its parameters.

    Parameters
    ----------
    module : GenericM
        The module.

    Returns
    -------
    GenericM
        The module.

    Examples
    --------
    >>> m = nn.Linear(10, 2)
    >>> m = to_eval(m)

    """

    module.eval()
    set_requires_grad(module, False)
    return module


def scheduler_requires_metric(scheduler: Any) -> bool:
    """
    Check if a scheduler requires a metric.

    Parameters
    ----------
    scheduler : Any
        The scheduler.

    Returns
    -------
    bool
        True if the scheduler requires a metric, False otherwise.

    Examples
    --------
    >>> scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(torch.optim.SGD(m.parameters(), lr=0.1))
    >>> scheduler_requires_metric(scheduler)
    True

    """

    return check_requires(scheduler.step, "metrics")


# This is a modified version of https://github.com/sksq96/pytorch-summary
#  So it can summary `carefree-learn` model structures better
def summary(
    m: nn.Module,
    sample_batch: tensor_dict_type,
    *,
    return_only: bool = False,
    summary_forward: Optional[Callable[[tensor_dict_type], None]] = None,
) -> str:
    """
    Print a summary of a module.

    Parameters
    ----------
    m : nn.Module
        The module.
    sample_batch : tensor_dict_type
        A sample batch of input to the module.
    return_only : bool, optional
        Whether to return the summary as a string instead of printing it. Default is False.
    summary_forward : Optional[Callable[[tensor_dict_type], None]], optional
        A function that takes a batch of input and passes it through the module. If None, the module's forward method is used.

    Returns
    -------
    str
        The summary of the module.

    Examples
    --------
    >>> m = nn.Linear(10, 2)
    >>> sample_batch = {"input": torch.randn(1, 10)}
    >>> print(summary(m, sample_batch, return_only=True, summary_forward=lambda x: m(x["input"])))
    ========================================================================================================================
    Layer (type)                             Input Shape                             Output Shape    Trainable Param #
    ------------------------------------------------------------------------------------------------------------------------
    Linear                                      [-1, 10]                                  [-1, 2]                   22
    ========================================================================================================================
    Total params: 22
    Trainable params: 22
    Non-trainable params: 0
    ------------------------------------------------------------------------------------------------------------------------
    Input size (MB): 0.00
    Forward/backward pass size (MB): 0.00
    Params size (MB): 0.00
    Estimated Total Size (MB): 0.00
    ------------------------------------------------------------------------------------------------------------------------

    """

    def _get_param_counts(m: nn.Module) -> Tuple[int, int]:
        num_params = 0
        num_trainable_params = 0
        for p in m.parameters():
            local_num_params = int(round(prod(p.data.shape)))
            num_params += local_num_params
            if p.requires_grad:
                num_trainable_params += local_num_params
        return num_params, num_trainable_params

    def register_hook(m: nn.Module) -> None:
        def inject_output_shape(output: Any, res: Dict[int, Any]) -> None:
            idx = 0 if not res else max(res)
            if isinstance(output, Tensor):
                o_shape = list(output.shape)
                if o_shape:
                    o_shape[0] = -1
                res[idx + 1] = o_shape
                return
            if isinstance(output, (list, tuple)):
                o_res = res[idx + 1] = {}
                for o in output:
                    inject_output_shape(o, o_res)

        def hook(m: nn.Module, inp: Any, output: Any) -> None:
            m_name = module_names.get(m)
            if m_name is None:
                return

            if not inp:
                return
            inp = inp[0]
            if not isinstance(inp, Tensor):
                return

            m_dict: OrderedDict[str, Any] = OrderedDict()
            m_dict["input_shape"] = list(inp.shape)
            if len(m_dict["input_shape"]) > 0:
                m_dict["input_shape"][0] = -1
            output_shape_res = m_dict["output_shape"] = {}
            inject_output_shape(output, output_shape_res)

            num_params_, num_trainable_params_ = _get_param_counts(m)
            m_dict["num_params"] = num_params_
            m_dict["num_trainable_params"] = num_trainable_params_
            raw_summary_dict[m_name] = m_dict

        if not isinstance(m, torch.jit.ScriptModule):
            hooks.append(m.register_forward_hook(hook))

    # get names
    def _inject_names(m: nn.Module, previous_names: List[str]) -> None:
        info_list = []
        for child in m.children():
            current_names = previous_names + [type(child).__name__]
            current_name = ".".join(current_names)
            module_names[child] = current_name
            info_list.append((child, current_name, current_names))
        counts: Dict[str, int] = defaultdict(int)
        idx_mapping: Dict[nn.Module, int] = {}
        for child, current_name, _ in info_list:
            idx_mapping[child] = counts[current_name]
            counts[current_name] += 1
        for child, current_name, current_names in info_list:
            if counts[current_name] == 1:
                continue
            current_name = f"{current_name}-{idx_mapping[child]}"
            module_names[child] = current_name
            current_names[-1] = current_name.split(".")[-1]
        for child, _, current_names in info_list:
            _inject_names(child, current_names)

    module_names: OrderedDict[nn.Module, str] = OrderedDict()
    existing_names: Set[str] = set()

    def _get_name(original: str) -> str:
        count = 0
        final_name = original
        while final_name in existing_names:
            count += 1
            final_name = f"{original}_{count}"
        existing_names.add(final_name)
        return final_name

    model_name = _get_name(type(m).__name__)
    module_names[m] = model_name
    _inject_names(m, [model_name])

    # create properties
    raw_summary_dict: OrderedDict[str, Any] = OrderedDict()
    hooks: List[Any] = []

    # register hook
    m.apply(register_hook)

    # make a forward pass
    with eval_context(m, use_grad=None):
        (summary_forward or m)(sample_batch)
        for param in m.parameters():
            param.grad = None

    # remove these hooks
    for h in hooks:
        h.remove()

    # get hierarchy
    hierarchy: OrderedDict[str, Any] = OrderedDict()
    for key in raw_summary_dict:
        split = key.split(".")
        d = hierarchy
        for elem in split[:-1]:
            d = d.setdefault(elem, OrderedDict())
        d.setdefault(split[-1], None)

    # reconstruct summary_dict
    def _inject_summary(current_hierarchy: Any, previous_keys: List[str]) -> None:
        if previous_keys and not previous_keys[-1]:
            previous_keys.pop()
        current_layer = len(previous_keys)
        current_count = hierarchy_counts.get(current_layer, 0)
        prefix = "  " * current_layer
        for k, v in current_hierarchy.items():
            current_keys = previous_keys + [k]
            concat_k = ".".join(current_keys)
            current_summary = raw_summary_dict.get(concat_k)
            summary_dict[f"{prefix}{k}-{current_count}"] = current_summary
            hierarchy_counts[current_layer] = current_count + 1
            if v is not None:
                _inject_summary(v, current_keys)

    hierarchy_counts: Dict[int, int] = {}
    summary_dict: OrderedDict[str, Any] = OrderedDict()
    _inject_summary(hierarchy, [])

    line_length = 120
    messages = ["=" * line_length]
    line_format = "{:30}  {:>20} {:>40} {:>20}"
    headers = "Layer (type)", "Input Shape", "Output Shape", "Trainable Param #"
    messages.append(line_format.format(*headers))
    messages.append("-" * line_length)
    total_output = 0
    for layer, layer_summary in summary_dict.items():
        layer_name = "-".join(layer.split("-")[:-1])
        valid_layer_name = layer_name.strip()
        num_spaces = len(layer_name) - len(valid_layer_name)
        valid_layer_name = truncate_string_to_length(valid_layer_name, 30 - num_spaces)
        layer_name = " " * num_spaces + valid_layer_name
        if layer_summary is None:
            messages.append(line_format.format(layer_name, "", "", ""))
        else:
            is_title = True
            all_output_shapes: List[List[int]] = []

            def _inject(output_shape_item: Dict[int, Any], prefix: str) -> None:
                only_one = len(output_shape_item) == 1
                for i, idx in enumerate(sorted(output_shape_item)):
                    if not prefix and only_one:
                        idx_prefix = ""
                    else:
                        idx_prefix = f"{prefix}{idx}."
                    value = output_shape_item[idx]
                    if isinstance(value, dict):
                        _inject(value, idx_prefix)
                        continue
                    output_shape_str = f"{idx_prefix} {str(value):>16s}"
                    ntp_str = "{0:,}".format(layer_summary["num_trainable_params"])
                    nonlocal is_title
                    messages.append(
                        line_format.format(
                            layer_name if is_title else "",
                            str(layer_summary["input_shape"]) if is_title else "",
                            output_shape_str,
                            ntp_str if is_title else "",
                        )
                    )
                    is_title = False
                    all_output_shapes.append(value)

            _inject(layer_summary["output_shape"], "")
            for shape in all_output_shapes:
                total_output += prod(shape)

    total_params, trainable_params = _get_param_counts(m)
    # assume 4 bytes/number (float on cuda).
    x_batch = sample_batch[INPUT_KEY]
    get_size = lambda t: abs(prod(t.shape[1:]) * 4.0 / (1024**2.0))
    if not isinstance(x_batch, list):
        x_batch = [x_batch]
    total_input_size = sum(map(get_size, x_batch))
    # x2 for gradients
    total_output_size = abs(2.0 * total_output * 4.0 / (1024**2.0))
    total_params_size = abs(total_params * 4.0 / (1024**2.0))
    total_size = total_params_size + total_output_size + total_input_size

    non_trainable_params = total_params - trainable_params
    messages.append("=" * line_length)
    messages.append("Total params: {0:,}".format(total_params))
    messages.append("Trainable params: {0:,}".format(trainable_params))
    messages.append("Non-trainable params: {0:,}".format(non_trainable_params))
    messages.append("-" * line_length)
    messages.append("Input size (MB): %0.2f" % total_input_size)
    messages.append("Forward/backward pass size (MB): %0.2f" % total_output_size)
    messages.append("Params size (MB): %0.2f" % total_params_size)
    messages.append("Estimated Total Size (MB): %0.2f" % total_size)
    messages.append("-" * line_length)
    msg = "\n".join(messages)
    if not return_only:
        print(msg)
    return msg


class DDPInfo(NamedTuple):
    """
    A named tuple for storing Distributed Data Parallel (DDP) information.

    Attributes
    ----------
    rank : int
        The rank of the current process in the DDP group.
    world_size : int
        The total number of processes in the DDP group.
    local_rank : int
        The rank of the current process within its machine.

    """

    rank: int
    world_size: int
    local_rank: int


def get_ddp_info() -> Optional[DDPInfo]:
    """
    Get DDP information from the environment variables.

    Returns
    -------
    Optional[DDPInfo]
        The DDP information if the relevant environment variables are set, otherwise None.

    Examples
    --------
    >>> get_ddp_info()

    """

    if "RANK" in os.environ and "WORLD_SIZE" in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ["WORLD_SIZE"])
        local_rank = int(os.environ["LOCAL_RANK"])
        return DDPInfo(rank, world_size, local_rank)
    return None


def is_local_rank_0() -> bool:
    """
    Check if the local rank is 0.

    Returns
    -------
    bool
        True if the local rank is 0 or DDP is not being used, False otherwise.

    Examples
    --------
    >>> is_local_rank_0()
    True

    """

    ddp_info = get_ddp_info()
    return ddp_info is None or ddp_info.local_rank == 0


def get_world_size() -> int:
    """
    Get the world size.

    Returns
    -------
    int
        The world size if DDP is being used, otherwise 1.

    Examples
    --------
    >>> get_world_size()
    1

    """

    ddp_info = get_ddp_info()
    return 1 if ddp_info is None else ddp_info.world_size


class toggle_optimizer:
    """
    A context manager for only enabling the gradients of a module for a specific optimizer,
    and disabling the gradients of other parameters.

    Parameters
    ----------
    m : nn.Module
        The module to toggle the gradients of.
    optimizer : Optimizer
        The optimizer.
    enabled : bool, optional
        Whether to enable this context manager. Default is True.

    Examples
    --------
    >>> m = nn.Linear(10, 2)
    >>> optimizer = torch.optim.SGD([m.weight], lr=0.1)
    >>> with toggle_optimizer(m, optimizer):
    ...     print(m.weight.requires_grad)  # True
    ...     print(m.bias.requires_grad)  # False
    >>> print(m.weight.requires_grad)  # True
    >>> print(m.bias.requires_grad)  # True

    """

    def __init__(self, m: nn.Module, optimizer: Optimizer, *, enabled: bool = True):
        self.m = m
        self.optimizer = optimizer
        self.enabled = enabled
        self.requires_grad: Dict[str, bool] = {}

    def __enter__(self) -> None:
        if not self.enabled:
            return
        self.requires_grad = {k: p.requires_grad for k, p in self.m.named_parameters()}
        for p in self.m.parameters():
            p.requires_grad = False
        for group in self.optimizer.param_groups:
            for p in group["params"]:
                p.requires_grad = True

    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        if not self.enabled:
            return
        for k, p in self.m.named_parameters():
            requires_grad = self.requires_grad.get(k)
            if requires_grad is not None:
                p.requires_grad = requires_grad


class mode_context:
    """
    A context manager for temporarily setting the mode of a module and
    optionally enabling or disabling gradient computation.

    Parameters
    ----------
    module : nn.Module
        The module to set the mode of.
    to_train : Optional[bool]
        Whether to set the module to training mode.
        If None, the mode of the module is not changed.
    use_grad : Optional[bool]
        Whether to enable gradient computation.
        If None, the `requires_grad` attribute of the parameters of the module is not changed.
    use_inference : Optional[bool]
        Whether to enable inference mode.
        If None, inference mode is not used.

    Examples
    --------
    >>> m = nn.Linear(10, 2)
    >>> with mode_context(m, to_train=False, use_grad=False):
    ...     print(m.training, any(p.requires_grad for p in m.parameters()))
    False False
    >>> print(m.training, any(p.requires_grad for p in m.parameters()))
    True True

    """

    def __init__(
        self,
        module: nn.Module,
        *,
        to_train: Optional[bool],
        use_grad: Optional[bool],
        use_inference: Optional[bool] = None,
    ):
        self._to_train = to_train
        self._module, self._training = module, module.training
        self._cache = {p: p.requires_grad for p in module.parameters()}
        if use_grad is not None:
            for p in module.parameters():
                p.requires_grad_(use_grad)
        if use_grad is None:
            self._grad_context: Optional[ContextManager] = None
        else:
            self._grad_context = torch.enable_grad() if use_grad else torch.no_grad()
        if use_inference is None:
            self._inference_context: Optional[ContextManager] = None
        else:
            self._inference_context = torch.inference_mode(use_inference)

    def __enter__(self) -> None:
        if self._to_train is not None:
            self._module.train(mode=self._to_train)
        if self._grad_context is not None:
            self._grad_context.__enter__()
        if self._inference_context is not None:
            self._inference_context.__enter__()

    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        if self._to_train is not None:
            self._module.train(mode=self._training)
        if self._inference_context is not None:
            self._inference_context.__exit__(exc_type, exc_val, exc_tb)
        if self._grad_context is not None:
            self._grad_context.__exit__(exc_type, exc_val, exc_tb)
        for p, v in self._cache.items():
            if p.requires_grad != v:
                p.requires_grad_(v)


class train_context(mode_context):
    """
    A context manager for temporarily setting a module to training mode and
    optionally enabling gradient computation.

    Parameters
    ----------
    module : nn.Module
        The module to set to training mode.
    use_grad : bool, optional
        Whether to enable gradient computation. Default is True.

    Examples
    --------
    >>> m = nn.Linear(10, 2)
    >>> m.eval()
    >>> with train_context(m):
    ...     print(m.training)
    True
    >>> print(m.training)
    False

    """

    def __init__(self, module: nn.Module, *, use_grad: bool = True):
        super().__init__(module, to_train=True, use_grad=use_grad, use_inference=False)


class eval_context(mode_context):
    """
    A context manager for temporarily setting a module to evaluation mode
    and optionally disabling gradient computation.

    Parameters
    ----------
    module : nn.Module
        The module to set to evaluation mode.
    use_grad : Optional[bool], optional
        Whether to enable gradient computation. Default is False.
    use_inference : Optional[bool], optional
        Whether to enable inference mode.
        If None and `use_grad` is not None, inference mode is set to the opposite of `use_grad`.

    Examples
    --------
    >>> m = nn.Linear(10, 2)
    >>> m.train()
    >>> with eval_context(m):
    ...     print(m.training)
    False
    >>> print(m.training)
    True

    """

    def __init__(
        self,
        module: nn.Module,
        *,
        use_grad: Optional[bool] = False,
        use_inference: Optional[bool] = None,
    ):
        if use_inference is None and use_grad is not None:
            use_inference = not use_grad
        super().__init__(
            module,
            to_train=False,
            use_grad=use_grad,
            use_inference=use_inference,
        )


class no_grad_context:
    """
    A context manager for disabling gradient computation.

    Parameters
    ----------
    enabled : bool
        Whether to enable this context manager.

    Examples
    --------
    >>> x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
    >>> with no_grad_context(enabled=True):
    ...     y = x ** 2
    >>> y.requires_grad
    False

    """

    def __init__(self, *, enabled: bool):
        self.enabled = enabled
        self._context = torch.no_grad()

    def __enter__(self) -> None:
        if not self.enabled:
            return
        self._context.__enter__()

    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        if not self.enabled:
            return
        self._context.__exit__(exc_type, exc_val, exc_tb)


class Initializer:
    """
    A class for initializing the parameters of a module.

    Parameters
    ----------
    config : Optional[Dict[str, Any]], optional
        A dictionary of configuration options.
        Default is None, which means an empty dictionary is used.

    Attributes
    ----------
    defined_initialization : set
        A set of the names of the defined initialization methods.
    custom_initializer : Dict[str, Callable]
        A dictionary mapping the names of custom initialization methods to the methods themselves.

    Examples
    --------
    >>> initializer = Initializer()
    >>> m = nn.Linear(10, 2)
    >>> initializer.initialize(m.weight, "xavier_uniform")

    """

    defined_initialization = {
        "xavier_uniform",
        "xavier_normal",
        "normal",
        "truncated_normal",
    }
    custom_initializer: Dict[str, Callable] = {}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self._verbose_level = self.config.setdefault("verbose_level", 2)

    def initialize(self, param: param_type, method: str) -> Any:
        """
        Initialize a parameter using a specified method.

        Parameters
        ----------
        param : param_type
            The parameter to initialize.
        method : str
            The name of the initialization method to use.

        Returns
        -------
        Any
            The result of the initialization method.

        Examples
        --------
        >>> initializer = Initializer()
        >>> m = nn.Linear(10, 2)
        >>> initializer.initialize(m.weight, "xavier_uniform")

        """

        custom_initializer = self.custom_initializer.get(method)
        if custom_initializer is None:
            return getattr(self, method)(param)
        return custom_initializer(self, param)

    @classmethod
    def register(cls, name: str) -> Callable[[Callable], Callable]:
        """
        Register a custom initialization method.

        Parameters
        ----------
        name : str
            The name of the initialization method to register.

        Returns
        -------
        Callable[[Callable], Callable]
            A decorator for registering the initialization method.

        Examples
        --------
        >>> @Initializer.register("custom")
        ... def custom(self, param):
        ...     with torch.no_grad():
        ...         param.data.fill_(1.0)

        """

        def _register(f: Callable) -> Callable:
            if name in cls.defined_initialization:
                raise ValueError(f"'{name}' initializer is already defined")
            cls.defined_initialization.add(name)
            cls.custom_initializer[name] = f
            return f

        return _register

    def xavier_uniform(self, param: param_type) -> None:
        """
        Initialize a parameter using the Xavier uniform initialization method.

        Parameters
        ----------
        param : param_type
            The parameter to initialize.

        Examples
        --------
        >>> initializer = Initializer()
        >>> m = nn.Linear(10, 2)
        >>> initializer.xavier_uniform(m.weight)

        """

        gain = self.config.setdefault("gain", 1.0)
        nn.init.xavier_uniform_(param.data, gain)

    def xavier_normal(self, param: param_type) -> None:
        """
        Initialize a parameter using the Xavier normal initialization method.

        Parameters
        ----------
        param : param_type
            The parameter to initialize.

        Examples
        --------
        >>> initializer = Initializer()
        >>> m = nn.Linear(10, 2)
        >>> initializer.xavier_normal(m.weight)

        """

        gain = self.config.setdefault("gain", 1.0)
        nn.init.xavier_normal_(param.data, gain)

    def normal(self, param: param_type) -> None:
        """
        Initialize a parameter using the normal initialization method.

        Parameters
        ----------
        param : param_type
            The parameter to initialize.

        Examples
        --------
        >>> initializer = Initializer()
        >>> m = nn.Linear(10, 2)
        >>> initializer.normal(m.weight)

        """

        mean = self.config.setdefault("mean", 0.0)
        std = self.config.setdefault("std", 1.0)
        with torch.no_grad():
            param.data.normal_(mean, std)

    def truncated_normal(self, param: param_type) -> None:
        """
        Initialize a parameter using the truncated normal initialization method.

        Parameters
        ----------
        param : param_type
            The parameter to initialize.

        Examples
        --------
        >>> initializer = Initializer()
        >>> m = nn.Linear(10, 2)
        >>> initializer.truncated_normal(m.weight)

        """

        span = self.config.setdefault("span", 2.0)
        mean = self.config.setdefault("mean", 0.0)
        std = self.config.setdefault("std", 1.0)
        tol = self.config.setdefault("tol", 0.0)
        epoch = self.config.setdefault("epoch", 20)
        num_elem = param.numel()
        weight_base = param.new_empty(num_elem).normal_()
        get_invalid = lambda w: (w > span) | (w < -span)
        invalid = get_invalid(weight_base)
        success = False
        for _ in range(epoch):
            num_invalid = invalid.sum().item()
            if num_invalid / num_elem <= tol:
                success = True
                break
            with torch.no_grad():
                weight_base[invalid] = param.new_empty(num_invalid).normal_()
                invalid = get_invalid(weight_base)
        if not success:
            print_warning(
                "invalid ratio for truncated normal : "
                f"{invalid.to(torch.float32).mean():8.6f}, it might cause by "
                f"too little epoch ({epoch}) or too small tolerance ({tol})",
            )
        with torch.no_grad():
            param.data.copy_(weight_base.reshape(param.shape))
            param.data.mul_(std).add_(mean)

    def orthogonal(self, param: param_type) -> None:
        """
        Initialize a parameter using the orthogonal initialization method.

        Parameters
        ----------
        param : param_type
            The parameter to initialize.

        Examples
        --------
        >>> initializer = Initializer()
        >>> m = nn.Linear(10, 2)
        >>> initializer.orthogonal(m.weight)

        """

        gain = self.config.setdefault("gain", 1.0)
        nn.init.orthogonal_(param.data, gain)


class ONNX:
    """
    A class for making predictions with an ONNX model.

    Parameters
    ----------
    onnx_path : str
        The path to the ONNX model file.

    Attributes
    ----------
    ort_session : InferenceSession
        The ONNX Runtime inference session.
    output_names : List[str]
        The names of the output nodes of the model.

    Raises
    ------
    ValueError
        If the `onnxruntime` package is not installed or `onnx_path` is not provided.

    Examples
    --------
    >>> model = ONNX("model.onnx")
    >>> inputs = {"input": np.array([1, 2, 3])}
    >>> outputs = model.predict(inputs)
    >>> print(outputs)
    {'output': array([2., 4., 6.])}

    """

    def __init__(self, onnx_path: str):
        if InferenceSession is None:
            msg = "`ONNX` is not available when `onnxruntime` is not installed"
            raise ValueError(msg)
        self.ort_session = InferenceSession(onnx_path)
        self.output_names = [node.name for node in self.ort_session.get_outputs()]

    def predict(self, new_inputs: np_dict_type) -> np_dict_type:
        """
        Make a prediction with the ONNX model.

        Parameters
        ----------
        new_inputs : np_dict_type
            A dictionary mapping input node names to input data.

        Returns
        -------
        np_dict_type
            A dictionary mapping output node names to output data.

        Examples
        --------
        >>> model = ONNX("model.onnx")
        >>> inputs = {"input": np.array([1, 2, 3])}
        >>> outputs = model.predict(inputs)
        >>> print(outputs)
        {'output': array([2., 4., 6.])}

        """

        ort_inputs = {
            node.name: to_standard(new_inputs[node.name])
            for node in self.ort_session.get_inputs()
        }
        return dict(zip(self.output_names, self.ort_session.run(None, ort_inputs)))


def gradient_checkpoint(
    func: Callable,
    inputs: Iterable[Tensor],
    params: Iterable[Union[Tensor, nn.Parameter]],
    enabled: bool,
) -> Union[Tensor, Iterable[Tensor]]:
    """
    Apply gradient checkpointing to a function.

    Gradient checkpointing is a technique to reduce the memory consumption of
    backpropagation. Instead of storing all intermediate activations of the entire
    computation graph for computing backward, the checkpointed part does not save
    intermediate activations, and instead recomputes them in backward pass. It can be
    applied on any part of a model.

    Parameters
    ----------
    func : Callable
        The function to apply gradient checkpointing to.
    inputs : Iterable[Tensor]
        The inputs to the function.
    params : Iterable[Union[Tensor, nn.Parameter]]
        The parameters of the function.
    enabled : bool
        Whether to enable gradient checkpointing.

    Returns
    -------
    Union[Tensor, Iterable[Tensor]]
        The result of the function.

    Examples
    --------
    >>> def func(x, y):
    ...     return [x * y]
    >>> gradient_checkpoint(func, [torch.tensor(2.0), torch.tensor(3.0)], [], True)
    [tensor(6.)]

    """

    if not enabled:
        return func(*inputs)
    inputs = tuple(inputs)
    args = inputs + tuple(params)
    return GradientCheckpointFunction.apply(func, len(inputs), *args)


class GradientCheckpointFunction(torch.autograd.Function):
    """
    A torch.autograd.Function subclass for gradient checkpointing.

    This class is used internally by the `gradient_checkpoint` function
    and should not be used directly.

    Examples
    --------
    >>> # This class is used internally and should not be used directly.

    """

    @staticmethod
    def forward(ctx: Any, *args: Any, **kwargs: Any) -> Any:
        run_function, length, *args = args  # type: ignore
        ctx.run_function = run_function
        ctx.input_tensors = list(args[:length])
        ctx.input_params = list(args[length:])
        ctx.grad_requires = [x.requires_grad for x in ctx.input_tensors]

        with torch.no_grad():
            output_tensors = ctx.run_function(*ctx.input_tensors)
        return output_tensors

    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) -> Any:
        input_tensors = [
            x.detach().requires_grad_(r)
            for x, r in zip(ctx.input_tensors, ctx.grad_requires)
        ]
        input_params = ctx.input_params
        any_tensors_fp16 = any(x.dtype == torch.float16 for x in input_tensors)
        any_params_fp16 = any(x.dtype == torch.float16 for x in input_params)
        enable_autocast = any_tensors_fp16 or any_params_fp16
        with torch.enable_grad(), torch.cuda.amp.autocast(enabled=enable_autocast):
            shallow_copies = [x.view_as(x) for x in input_tensors]
            output_tensors = ctx.run_function(*shallow_copies)
        tensors_indices = [i for i, x in enumerate(input_tensors) if x.requires_grad]
        params_indices = [i for i, x in enumerate(input_params) if x.requires_grad]
        grad_tensors = [input_tensors[i] for i in tensors_indices]
        grad_params = [input_params[i] for i in params_indices]
        input_grads = torch.autograd.grad(
            output_tensors,
            grad_tensors + grad_params,
            grad_outputs,
            allow_unused=True,
        )
        n_grad_tensors = len(tensors_indices)
        n_tensors = len(input_tensors)
        n_params = len(input_params)
        del ctx.input_tensors
        del ctx.input_params
        del input_tensors
        del input_params
        del grad_tensors
        del grad_params
        del grad_outputs
        del shallow_copies
        del output_tensors
        output_grads = [None] * (n_tensors + n_params)
        for i, idx in enumerate(tensors_indices):
            output_grads[idx] = input_grads[i]
        for i, idx in enumerate(params_indices):
            output_grads[n_tensors + idx] = input_grads[n_grad_tensors + i]
        return (None, None) + tuple(output_grads)


# ml


def to_2d(arr: data_type) -> data_type:
    """
    Convert an array-like object to a 2D array.

    If `arr` is already a 2D array or a list of lists, it is returned as is.
    If `arr` is a 1D array or list, it is reshaped to a 2D array with one column.
    If `arr` is None or a string, None is returned.

    Parameters
    ----------
    arr : data_type
        The array-like object to convert.

    Returns
    -------
    data_type
        The 2D array.

    Examples
    --------
    >>> to_2d(np.array([1, 2, 3]))
    array([[1],
           [2],
           [3]])
    >>> to_2d([[1, 2], [3, 4]])
    [[1, 2], [3, 4]]
    >>> to_2d(None)
    >>> to_2d("string")

    """

    if arr is None or isinstance(arr, str):
        return None
    if isinstance(arr, np.ndarray):
        return arr.reshape([len(arr), -1])
    if isinstance(arr[0], list):
        return arr
    return [[elem] for elem in arr]  # type: ignore


# cv


def auto_num_layers(
    img_size: int,
    min_size: int = 4,
    target_layers: Optional[int] = 4,
    *,
    use_stride: bool = False,
) -> int:
    """
    Automatically determine the number of layers for a model based on the image size.

    Parameters
    ----------
    img_size : int
        The size of the input images.
    min_size : int, optional
        The minimum size of the feature maps in the model. Default is 4.
    target_layers : Optional[int], optional
        The target number of layers. If None, the maximum number of layers is used. Default is 4.
    use_stride : bool, optional
        Whether to use stride to reduce the size of the feature maps.
        If True, the number of layers is rounded up; otherwise, it is rounded down. Default is False.

    Returns
    -------
    int
        The number of layers.

    Examples
    --------
    >>> auto_num_layers(64, min_size=4, target_layers=4, use_stride=False)
    4

    """

    fn = math.ceil if use_stride else math.floor
    max_layers = fn(math.log2(img_size / min_size))
    if target_layers is None:
        return max_layers
    return max(2, min(target_layers, max_layers))


def slerp(
    x1: torch.Tensor,
    x2: torch.Tensor,
    r1: Union[float, torch.Tensor],
    r2: Optional[Union[float, torch.Tensor]] = None,
    *,
    dot_threshold: float = 0.9995,
) -> torch.Tensor:
    """
    Perform spherical linear interpolation (slerp) between two tensors.

    Parameters
    ----------
    x1 : torch.Tensor
        The first tensor, it should be at least 2D.
    x2 : torch.Tensor
        The second tensor, it should be at least 2D.
    r1 : Union[float, torch.Tensor]
        The interpolation factor for the first tensor.
    r2 : Optional[Union[float, torch.Tensor]], optional
        The interpolation factor for the second tensor. If None, it is set to `1.0 - r1`. Default is None.
    dot_threshold : float, optional
        The dot product threshold for the overflow condition. Default is 0.9995.

    Returns
    -------
    torch.Tensor
        The interpolated tensor.

    Examples
    --------
    >>> x1 = torch.tensor([[1.0, 0.0]])
    >>> x2 = torch.tensor([[0.0, 1.0]])
    >>> slerp(x1, x2, 0.5)
    tensor([[0.7071, 0.7071]])

    """

    if r2 is None:
        r2 = 1.0 - r1
    b, *shape = x1.shape
    x1 = x1.view(b, -1)
    x2 = x2.view(b, -1)
    low_norm = x1 / torch.norm(x1, dim=1, keepdim=True)
    high_norm = x2 / torch.norm(x2, dim=1, keepdim=True)
    dot = (low_norm * high_norm).sum(1)
    overflow_mask = dot > dot_threshold
    out = torch.zeros_like(x1)
    out[overflow_mask] = r1 * x1 + r2 * x2
    normal_mask = ~overflow_mask
    omega = torch.acos(dot[normal_mask])
    so = torch.sin(omega)
    x1_part = (torch.sin(r1 * omega) / so).unsqueeze(1) * x1
    x2_part = (torch.sin(r2 * omega) / so).unsqueeze(1) * x2
    out[normal_mask] = x1_part + x2_part
    return out.view(b, *shape)


def interpolate(
    src: Tensor,
    *,
    mode: str = "nearest",
    factor: Optional[Union[float, Tuple[float, float]]] = None,
    size: Optional[Union[int, Tuple[int, int]]] = None,
    anchor: Optional[Tensor] = None,
    deterministic: bool = False,
    **kwargs: Any,
) -> Tensor:
    """
    Interpolate a tensor to a specified size or by a specified factor.

    Parameters
    ----------
    src : Tensor
        The source tensor.
    mode : str, optional
        The interpolation mode. Default is "nearest".
    factor : Optional[Union[float, Tuple[float, float]]], optional
        The scale factor for the interpolation.
        If provided, `size` and `anchor` will be ignored. Default is None.
    size : Optional[Union[int, Tuple[int, int]]], optional
        The target size for the interpolation.
        If `factor` is not provided, either `size` or `anchor` must be provided. Default is None.
    anchor : Optional[Tensor], optional
        A tensor whose size will be used as the target size for the interpolation.
        If `factor` is not provided, either `size` or `anchor` must be provided. Default is None.
    deterministic : bool, optional
        Whether to use deterministic computation. Default is False.
    **kwargs : Any
        Additional keyword arguments to pass to the interpolation function.

    Returns
    -------
    Tensor
        The interpolated tensor.

    Examples
    --------
    >>> src = torch.rand(1, 3, 4, 4)
    >>> interpolate(src, mode="bilinear", size=(8, 8))

    """

    if "linear" in mode or mode == "bicubic":
        kwargs.setdefault("align_corners", False)
    c, h, w = src.shape[1:]
    if deterministic:
        c, h, w = map(int, [c, h, w])
    if factor is not None:
        template = "`{}` will take no affect because `factor` is provided"
        if size is not None:
            print_warning(template.format("size"))
        if anchor is not None:
            print_warning(template.format("anchor"))
        if factor == 1.0 or factor == (1.0, 1.0):
            return src
        if not deterministic:
            return F.interpolate(
                src,
                mode=mode,
                scale_factor=factor,
                recompute_scale_factor=True,
                **kwargs,
            )
        if not isinstance(factor, tuple):
            factor = factor, factor
        size = tuple(map(int, map(round, [h * factor[0], w * factor[1]])))  # type: ignore
    if size is None:
        if anchor is None:
            raise ValueError("either `size` or `anchor` should be provided")
        size = anchor.shape[2:]
        if deterministic:
            size = tuple(map(int, size))  # type: ignore
    if not isinstance(size, tuple):
        size = size, size
    if h == size[0] and w == size[1]:
        return src
    net = F.interpolate(src, size=size, mode=mode, **kwargs)
    if not deterministic:
        return net
    return net.view(-1, c, *size)


def mean_std(
    latent_map: Tensor,
    eps: float = 1.0e-5,
    *,
    deterministic: bool = False,
) -> Tuple[Tensor, Tensor]:
    """
    Compute the mean and standard deviation of a latent map.

    Parameters
    ----------
    latent_map : Tensor
        The latent map.
    eps : float, optional
        A small constant for numerical stability. Default is 1.0e-5.
    deterministic : bool, optional
        Whether to use deterministic computation. Default is False.

    Returns
    -------
    Tuple[Tensor, Tensor]
        The mean and standard deviation of the latent map.

    Examples
    --------
    >>> latent_map = torch.rand(1, 3, 4, 4)
    >>> mean_std(latent_map)

    """

    c, h, w = latent_map.shape[1:]
    if deterministic:
        c, h, w = map(int, [c, h, w])
    spatial_dim = h * w
    latent_var = latent_map.view(-1, c, spatial_dim).var(dim=2) + eps
    latent_std = latent_var.sqrt().view(-1, c, 1, 1)
    latent_mean = latent_map.view(-1, c, spatial_dim).mean(dim=2).view(-1, c, 1, 1)
    return latent_mean, latent_std


def adain_with_params(
    src: Tensor,
    mean: Tensor,
    std: Tensor,
    *,
    deterministic: bool = False,
) -> Tensor:
    """
    Apply Adaptive Instance Normalization (AdaIN) to a source tensor
    using provided mean and standard deviation.

    Parameters
    ----------
    src : Tensor
        The source tensor.
    mean : Tensor
        The mean for AdaIN.
    std : Tensor
        The standard deviation for AdaIN.
    deterministic : bool, optional
        Whether to use deterministic computation. Default is False.

    Returns
    -------
    Tensor
        The tensor after AdaIN.

    Examples
    --------
    >>> src = torch.rand(1, 3, 4, 4)
    >>> mean = torch.rand(1, 3, 1, 1)
    >>> std = torch.rand(1, 3, 1, 1)
    >>> adain_with_params(src, mean, std)

    """

    src_mean, src_std = mean_std(src, deterministic=deterministic)
    src_normalized = (src - src_mean) / src_std
    return src_normalized * std + mean


def adain_with_tensor(
    src: Tensor,
    tgt: Tensor,
    *,
    deterministic: bool = False,
) -> Tensor:
    """
    Apply Adaptive Instance Normalization (AdaIN) to a source tensor
    using the mean and standard deviation of a target tensor.

    Parameters
    ----------
    src : Tensor
        The source tensor.
    tgt : Tensor
        The target tensor.
    deterministic : bool, optional
        Whether to use deterministic computation. Default is False.

    Returns
    -------
    Tensor
        The tensor after AdaIN.

    Examples
    --------
    >>> src = torch.rand(1, 3, 4, 4)
    >>> tgt = torch.rand(1, 3, 4, 4)
    >>> adain_with_tensor(src, tgt)

    """

    tgt_mean, tgt_std = mean_std(tgt, deterministic=deterministic)
    return adain_with_params(src, tgt_mean, tgt_std, deterministic=deterministic)


def make_indices_visualization_map(indices: Tensor) -> Tensor:
    """
    Create a visualization map for a tensor of indices.

    Each index is visualized as a white 28x28 image with the index number drawn in the center.

    Parameters
    ----------
    indices : Tensor
        The tensor of indices.

    Returns
    -------
    Tensor
        The visualization map.

    Examples
    --------
    >>> indices = torch.tensor([1, 2, 3])
    >>> make_indices_visualization_map(indices)

    """

    images = []
    for idx in indices.view(-1).tolist():
        img = Image.new("RGB", (28, 28), (250, 250, 250))
        draw = ImageDraw.Draw(img)
        draw.text((12, 9), str(idx), (0, 0, 0))
        images.append(to_torch(np.array(img).transpose([2, 0, 1])))
    return torch.stack(images).float()
