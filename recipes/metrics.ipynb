{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Metrics Recipes\n",
    "\n",
    "In this page, we will show you how to customize your own metrics. In `carefree-learn`, it is fairly easy to define various kinds of metrics (ML, CV, etc.) with a unified API.\n",
    "\n",
    "> You might notice that if you run the blocks with `register_metric` calls for more than once, `carefree-learn` will throw a warning which says \" '...' has already been registered \", and your changes will have no effect. This is intentional because normally we DO NOT want to register anything for more than once.\n",
    "> \n",
    "> However, if you are using some interactive developing tools (e.g. Jupyter Notebook), it is very common to modify the implementations for more than once. In this case, we can set `allow_duplicate=True` in the `register_*` functions to bypass this check. And of course, this should NEVER happen in production for safety!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "\n",
    "- [ML Metrics](#ML-Metrics)\n",
    "- [CV Metrics](#CV-Metrics)\n",
    "- [Integration](#Integration)\n",
    "  - [Single Metric](#Single-Metric)\n",
    "  - [Multiple Metrics](#Multiple-Metrics)\n",
    "  - [Weighted Metrics](#Weighted-Metrics)\n",
    "  - [Complex Metrics](#Complex-Metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You might also notice that:\n",
    "> - All classes have inherited `MetricInterface`. This is not required, but it can guide you to implement the essential parts in IDE.\n",
    "> - The class name somehow matches the registered name. This is also not required, since `carefree-learn` only cares about the name that you pass to the `register_*` functions, and will not check the actual class name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2164caac350>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import cflearn\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Dict\n",
    "from cftool.array import iou\n",
    "from cftool.array import corr\n",
    "from cftool.array import softmax\n",
    "\n",
    "try:\n",
    "    from sklearn import metrics\n",
    "except:\n",
    "    metrics = None\n",
    "\n",
    "np.random.seed(142857)\n",
    "torch.manual_seed(142857)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typical classification metric\n",
    "@cflearn.register_metric(\"my_binary_accuracy\", allow_duplicate=False)\n",
    "class MyBinaryAccuracy(cflearn.MetricInterface):\n",
    "    def __init__(self, threshold: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    # True means that the larger this metric is, the better.\n",
    "    @property\n",
    "    def is_positive(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    # logits : [N, 1]\n",
    "    # labels : [N, 1]\n",
    "    def forward(self, logits: np.ndarray, labels: np.ndarray) -> float:\n",
    "        predictions = (logits > self.threshold).astype(int)\n",
    "        return (predictions == labels).mean().item()\n",
    "\n",
    "# typical regression metric\n",
    "@cflearn.register_metric(\"my_l1\", allow_duplicate=False)\n",
    "class MyL1(cflearn.MetricInterface):\n",
    "    # False means that the smaller this metric is, the better.\n",
    "    @property\n",
    "    def is_positive(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    # predictions : [N, 1]\n",
    "    # labels      : [N, 1]\n",
    "    def forward(self, predictions: np.ndarray, labels: np.ndarray) -> float:\n",
    "        return np.abs(predictions - labels).mean().item()\n",
    "    \n",
    "# special classification metric, which requires the whole dataset to evaluate.\n",
    "@cflearn.register_metric(\"my_auc\", allow_duplicate=False)\n",
    "class MyAUC(cflearn.MetricInterface):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        if metrics is None:\n",
    "            print(\"`scikit-learn` needs to be installed for `AUC`\")\n",
    "\n",
    "    # True means that the larger this metric is, the better.\n",
    "    @property\n",
    "    def is_positive(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    #   True means that this metric requires the entire dataset to evaluate.\n",
    "    #   For AUC, this has to be True because for some imbalanced dataset, it is \n",
    "    # very likely to have some batches that only contain one kind of labels, which \n",
    "    # will crash the AUC calculation.\n",
    "    #   Notice that this is only useful when this metric is integrated in `carefree-learn`'s \n",
    "    # pipeline, where `carefree-learn` will read this flag and decide whether to pass\n",
    "    # the entire dataset to this metric or not.\n",
    "    @property\n",
    "    def requires_all(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    # K >= 2\n",
    "    # logits : [N, K]\n",
    "    # labels : [N, 1]\n",
    "    def forward(self, logits: np.ndarray, labels: np.ndarray) -> float:\n",
    "        if metrics is None:\n",
    "            return 0.0\n",
    "        num_classes = logits.shape[1]  # K\n",
    "        probabilities = softmax(logits)\n",
    "        labels = labels.ravel()\n",
    "        if num_classes == 2:\n",
    "            return metrics.roc_auc_score(labels, probabilities[..., 1])\n",
    "        return metrics.roc_auc_score(labels, probabilities, multi_class=\"ovr\")\n",
    "\n",
    "# special regression metric, which requires the whole dataset to evaluate.\n",
    "@cflearn.register_metric(\"my_corr\", allow_duplicate=False)\n",
    "class MyCorrelation(cflearn.MetricInterface):\n",
    "    # True means that the larger this metric is, the better.\n",
    "    @property\n",
    "    def is_positive(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    #   True means that this metric requires the entire dataset to evaluate.\n",
    "    #   For correlation, it is better to set it to True because correlation works better\n",
    "    # with more data. But this is kind of a trade-off: requiring the full dataset can\n",
    "    # indeed increase the accuracy of correlation estimation, but will also have much\n",
    "    # greater impact on your RAM.\n",
    "    @property\n",
    "    def requires_all(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    # predictions : [N, K]\n",
    "    # labels      : [N, K]\n",
    "    def forward(self, predictions: np.ndarray, labels: np.ndarray) -> float:\n",
    "        return corr(predictions, labels, get_diagonal=True).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.010000000000000007\n",
      "1.0\n",
      "0.9687798633063698\n",
      "1.0\n",
      "0.9999999999999996\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "logits = np.random.random([100, 1]) - 0.5\n",
    "labels = (logits > 0).astype(int)\n",
    "my_binary_accuracy = cflearn.api.make_metric(\"my_binary_accuracy\")\n",
    "print(my_binary_accuracy.core.forward(logits, labels))\n",
    "\n",
    "predictions = np.random.random([100, 1])\n",
    "labels = predictions - 0.01\n",
    "my_l1 = cflearn.api.make_metric(\"my_l1\")\n",
    "print(my_l1.core.forward(predictions, labels))\n",
    "\n",
    "logits = np.random.random([100, 2])\n",
    "labels = np.argmax(logits, axis=1, keepdims=True)\n",
    "my_auc = cflearn.api.make_metric(\"my_auc\")\n",
    "print(my_auc.core.forward(logits, labels))\n",
    "logits = np.random.random([100, 10])\n",
    "labels = np.argmax(logits, axis=1, keepdims=True)\n",
    "print(my_auc.core.forward(logits, labels))\n",
    "logits[range(100), labels.ravel()] += 1.0\n",
    "print(my_auc.core.forward(logits, labels))\n",
    "\n",
    "predictions = np.random.random([100, 1])\n",
    "labels = predictions - 0.01\n",
    "my_corr = cflearn.api.make_metric(\"my_corr\")\n",
    "print(my_corr.core.forward(predictions, labels))\n",
    "predictions = np.random.random([100, 10])\n",
    "labels = predictions - 0.01\n",
    "print(my_corr.core.forward(predictions, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Metrics\n",
    "\n",
    "> For **C**omputer **V**ision metrics, if the metric is image-based, we should never set its `requires_all` to `True` because it will be a disaster to put all your images to RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersection over Union, only supports binary situations\n",
    "@cflearn.register_metric(\"my_iou\", allow_duplicate=True)\n",
    "class MyIOU(cflearn.MetricInterface):\n",
    "    # True means that the larger this metric is, the better.\n",
    "    @property\n",
    "    def is_positive(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    # K âˆˆ {1, 2}\n",
    "    # logits : [N, K, H, W]\n",
    "    # labels : [N, 1, H, W]\n",
    "    def forward(self, logits: np.ndarray, labels: np.ndarray) -> float:\n",
    "        return iou(logits, labels).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38977155824353243\n",
      "0.5762673693012536\n",
      "0.9867113608015362\n",
      "0.9999092642198215\n",
      "1.0\n",
      "0.5762673693012536\n",
      "0.9867113608015362\n",
      "0.9999092642198215\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "logits = np.random.random([4, 2, 224, 224])\n",
    "labels = (logits[:, [1]] > 0.5).astype(int)\n",
    "concat = np.concatenate([1 - labels, labels], axis=1)\n",
    "my_iou = cflearn.api.make_metric(\"my_iou\")\n",
    "print(my_iou.core.forward(logits, labels))\n",
    "print(my_iou.core.forward(concat, labels))\n",
    "print(my_iou.core.forward(concat * 5, labels))\n",
    "print(my_iou.core.forward(concat * 10, labels))\n",
    "print(my_iou.core.forward(concat * 50, labels))\n",
    "print(my_iou.core.forward((labels - 0.5) * 2, labels))\n",
    "print(my_iou.core.forward((labels - 0.5) * 10, labels))\n",
    "print(my_iou.core.forward((labels - 0.5) * 20, labels))\n",
    "print(my_iou.core.forward((labels - 0.5) * 100, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration\n",
    "\n",
    "After defining our own metrics, we need to know how to integrate them in existing `carefree-learn` pipelines for training, testing and deploying. Basically, metrics could be specified across various APIs with `metric_names` and `metric_configs`. We will use `fit_ml` to demonstrate the core concepts, and the same recipes could be applied elsewhere.\n",
    "\n",
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random([100, 5])\n",
    "y = np.random.randint(0, 2, [100, 1])\n",
    "common_kwargs = dict(\n",
    "    x_train=x,\n",
    "    y_train=y,\n",
    "    x_valid=x,\n",
    "    y_valid=y,\n",
    "    core_name=\"linear\",\n",
    "    input_dim=5,\n",
    "    output_dim=1,\n",
    "    is_classification=True,\n",
    "    fixed_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Metric\n",
    "\n",
    "In this case, the `metric_names` should be an `str`, and the `metric_configs` should be the `kwargs` that will be passed to your metric's `__init__` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>> MyFooMetric.foo: 1.2345\n",
      "\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    Linear                                   [-1, 5]                                  [-1, 1]                    6\n",
      "      Linear                                 [-1, 5]                                  [-1, 1]                    6\n",
      "========================================================================================================================\n",
      "Total params: 6\n",
      "Trainable params: 6\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-23_13-00-17-676292\\checkpoints\n",
      "| epoch  -1  [-1 / 1] [0.066s] | my_foo_metric : 1.234500 | score : 1.234500 |\n"
     ]
    }
   ],
   "source": [
    "@cflearn.register_metric(\"my_foo_metric\", allow_duplicate=False)\n",
    "class MyFooMetric(cflearn.MetricInterface):\n",
    "    def __init__(self, foo):\n",
    "        super().__init__()\n",
    "        self.foo = foo\n",
    "        print(f\"\\n>>>>>> MyFooMetric.foo: {foo}\\n\")\n",
    "\n",
    "    @property\n",
    "    def is_positive(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def forward(self, logits, labels) -> float:\n",
    "        return self.foo\n",
    "\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    metric_names=\"my_foo_metric\",\n",
    "    metric_configs=dict(foo=1.2345),\n",
    "    **common_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You might notice that `carefree-learn` will print out the metrics and the final score of current model. The final score is simply a 'mean' of every metric, except that the metrics with `is_positive=False` will be the opposite during the calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>> MyFooNegativeMetric.foo: 1.2345\n",
      "\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    Linear                                   [-1, 5]                                  [-1, 1]                    6\n",
      "      Linear                                 [-1, 5]                                  [-1, 1]                    6\n",
      "========================================================================================================================\n",
      "Total params: 6\n",
      "Trainable params: 6\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-23_13-00-17-780291\\checkpoints\n",
      "| epoch  -1  [-1 / 1] [0.069s] | my_foo_negative_metric : 1.234500 | score : -1.23450 |\n"
     ]
    }
   ],
   "source": [
    "@cflearn.register_metric(\"my_foo_negative_metric\", allow_duplicate=False)\n",
    "class MyFooNegativeMetric(cflearn.MetricInterface):\n",
    "    def __init__(self, foo):\n",
    "        super().__init__()\n",
    "        self.foo = foo\n",
    "        print(f\"\\n>>>>>> MyFooNegativeMetric.foo: {foo}\\n\")\n",
    "\n",
    "    @property\n",
    "    def is_positive(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    def forward(self, logits, labels) -> float:\n",
    "        return self.foo\n",
    "\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    metric_names=\"my_foo_negative_metric\",\n",
    "    metric_configs=dict(foo=1.2345),\n",
    "    **common_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As shown above, the final score is now negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Metrics\n",
    "\n",
    "In this case, the `metric_names` should be a list of `str`, and the `metric_configs` should be a `dict`, where the keys are the names and the values will be passed to the corresponding `__init__` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>> MyFooMetric.foo: 1.2345\n",
      "\n",
      "\n",
      ">>>>>> MyBarMetric.bar: 2.3456\n",
      "\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    Linear                                   [-1, 5]                                  [-1, 1]                    6\n",
      "      Linear                                 [-1, 5]                                  [-1, 1]                    6\n",
      "========================================================================================================================\n",
      "Total params: 6\n",
      "Trainable params: 6\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-23_13-00-18-061030\\checkpoints\n",
      "| epoch  -1  [-1 / 1] [0.013s] | my_bar_metric : 2.345600 | my_foo_metric : 1.234500 | score : 1.790050 |\n"
     ]
    }
   ],
   "source": [
    "@cflearn.register_metric(\"my_bar_metric\", allow_duplicate=False)\n",
    "class MyFooMetric(cflearn.MetricInterface):\n",
    "    def __init__(self, bar):\n",
    "        super().__init__()\n",
    "        self.bar = bar\n",
    "        print(f\"\\n>>>>>> MyBarMetric.bar: {bar}\\n\")\n",
    "\n",
    "    @property\n",
    "    def is_positive(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def forward(self, logits, labels) -> float:\n",
    "        return self.bar\n",
    "\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    metric_names=[\"my_foo_metric\", \"my_bar_metric\"],\n",
    "    metric_configs=dict(\n",
    "        my_foo_metric=dict(foo=1.2345),\n",
    "        my_bar_metric=dict(bar=2.3456),\n",
    "    ),\n",
    "    **common_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Metrics\n",
    "\n",
    "`carefree-learn` also supports weighted metrics. The formula under the hood is:\n",
    "\n",
    "$$\n",
    "\\text{score}=\\frac1{\\sum_{i=1}^{k}w_i}\\cdot\\sum_{i=1}^{k}\\hat m_i(\\hat y, y)\\cdot w_i\n",
    "$$\n",
    "\n",
    "the default value of $w_i$ is $1$, and\n",
    "\n",
    "$$\n",
    "\\hat m_i(\\hat y, y)\\triangleq I(\\text m_i)\\cdot \\text{m}_i\\text{.forward}(\\hat y, y)\n",
    "$$\n",
    "\n",
    "where $\\text m_i$ is the $i$th metric, and\n",
    "\n",
    "$$\n",
    "I(\\text m_i)\\triangleq\n",
    "\\begin{cases}\n",
    " 1, & \\text{if m}_i\\text{.is_positive} \\\\\n",
    " -1, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In order to use weighted metrics, we can specify `metric_weights`, where the keys are the metric names and the values are the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>> MyFooMetric.foo: 1.2345\n",
      "\n",
      "\n",
      ">>>>>> MyBarMetric.bar: 2.3456\n",
      "\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    Linear                                   [-1, 5]                                  [-1, 1]                    6\n",
      "      Linear                                 [-1, 5]                                  [-1, 1]                    6\n",
      "========================================================================================================================\n",
      "Total params: 6\n",
      "Trainable params: 6\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-23_13-00-18-110115\\checkpoints\n",
      "| epoch  -1  [-1 / 1] [0.011s] | my_bar_metric : 2.345600 | my_foo_metric : 1.234500 | score : 1.962784 |\n",
      ">>> target score 1.9627840336134459\n"
     ]
    }
   ],
   "source": [
    "m = cflearn.api.fit_ml(\n",
    "    metric_names=[\"my_foo_metric\", \"my_bar_metric\"],\n",
    "    metric_configs=dict(\n",
    "        my_foo_metric=dict(foo=1.2345),\n",
    "        my_bar_metric=dict(bar=2.3456),\n",
    "    ),\n",
    "    metric_weights=dict(\n",
    "        my_foo_metric=0.123,\n",
    "        my_bar_metric=0.234,\n",
    "    ),\n",
    "    **common_kwargs,\n",
    ")\n",
    "\n",
    "print(\">>> target score\", (1.2345 * 0.123 + 2.3456 * 0.234) / (0.123 + 0.234))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Metrics\n",
    "\n",
    "In some complex situations, our inputs / labels may have multiple values (e.g. multi-task problems). `carefree-learn` supports your custom metrics receiving a `dict` of `np.ndarray`s for such cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    Linear                                   [-1, 5]                                  [-1, 1]                    6\n",
      "      Linear                                 [-1, 5]                                  [-1, 1]                    6\n",
      "========================================================================================================================\n",
      "Total params: 6\n",
      "Trainable params: 6\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "\n",
      ">>> detected prediction keys : ['predictions']\n",
      ">>> detected input      keys : ['input', 'labels', 'batch_indices', 'bar_input']\n",
      "\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-23_13-00-18-155115\\checkpoints\n",
      "\n",
      ">>> detected prediction keys : ['predictions']\n",
      ">>> detected input      keys : ['input', 'labels', 'batch_indices', 'bar_input']\n",
      "\n",
      "| epoch  -1  [-1 / 1] [0.014s] | my_complex_metric : 0.000000 | score : 0.000000 |\n"
     ]
    }
   ],
   "source": [
    "@cflearn.register_metric(\"my_complex_metric\", allow_duplicate=False)\n",
    "class MyComplexMetric(cflearn.MetricInterface):\n",
    "    @property\n",
    "    def is_positive(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        np_outputs: Dict[str, np.ndarray],\n",
    "        np_batch: Dict[str, np.ndarray],\n",
    "    ) -> float:\n",
    "        print(f\"\\n>>> detected prediction keys : {list(np_outputs.keys())}\")\n",
    "        print(f\">>> detected input      keys : {list(np_batch.keys())}\\n\")\n",
    "        return 0.0\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    train_others={\"foo_input\": np.random.random(x.shape)},\n",
    "    valid_others={\"bar_input\": np.random.random(x.shape)},\n",
    "    metric_names=\"my_complex_metric\",\n",
    "    **common_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You might notice that only `bar_input` is available. This is as expected because `carefree-learn` will use validation data to calculate the metrics. \n",
    "\n",
    "It's important to keep the first & second arguments **EXACTLY** as `np_outputs` & `np_batch`, because `carefree-learn` can therefore know that you require the full data instead of one single `np.ndarray`.\n",
    "\n",
    "You can also simplify your implementation with this design if you only require parts of the full data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    Linear                                   [-1, 5]                                  [-1, 1]                    6\n",
      "      Linear                                 [-1, 5]                                  [-1, 1]                    6\n",
      "========================================================================================================================\n",
      "Total params: 6\n",
      "Trainable params: 6\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "\n",
      ">>> detected predictions : (100, 1)\n",
      ">>> detected input keys  : ['input', 'labels', 'batch_indices', 'bar_input']\n",
      "\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-23_13-00-18-203113\\checkpoints\n",
      "\n",
      ">>> detected predictions : (100, 1)\n",
      ">>> detected input keys  : ['input', 'labels', 'batch_indices', 'bar_input']\n",
      "\n",
      "| epoch  -1  [-1 / 1] [0.013s] | my_complex_metric2 : 0.000000 | score : 0.000000 |\n"
     ]
    }
   ],
   "source": [
    "@cflearn.register_metric(\"my_complex_metric2\", allow_duplicate=False)\n",
    "class MyComplexMetric2(cflearn.MetricInterface):\n",
    "    @property\n",
    "    def is_positive(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        predictions: np.ndarray,\n",
    "        np_batch: Dict[str, np.ndarray],\n",
    "    ) -> float:\n",
    "        print(f\"\\n>>> detected predictions : {predictions.shape}\")\n",
    "        print(f\">>> detected input keys  : {list(np_batch.keys())}\\n\")\n",
    "        return 0.0\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    train_others={\"foo_input\": np.random.random(x.shape)},\n",
    "    valid_others={\"bar_input\": np.random.random(x.shape)},\n",
    "    metric_names=\"my_complex_metric2\",\n",
    "    **common_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some rare scenarios, we may even need the entire `DataLoader` to calculate our metrics. This is also accessible in `carefree-learn` by simply add a `loader` argument to the `forward` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    Linear                                   [-1, 5]                                  [-1, 1]                    6\n",
      "      Linear                                 [-1, 5]                                  [-1, 1]                    6\n",
      "========================================================================================================================\n",
      "Total params: 6\n",
      "Trainable params: 6\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "\n",
      ">>> loader      : <cflearn.data.core.MLLoader object at 0x000002167EA7EC40>\n",
      ">>> loader.data : <cflearn.data.core.MLDataset object at 0x000002167EFAEFD0>\n",
      "\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-23_13-00-18-248111\\checkpoints\n",
      "\n",
      ">>> loader      : <cflearn.data.core.MLLoader object at 0x000002167EA7EC40>\n",
      ">>> loader.data : <cflearn.data.core.MLDataset object at 0x000002167EFAEFD0>\n",
      "\n",
      "| epoch  -1  [-1 / 1] [0.012s] | my_metric_with_loader : 0.000000 | score : 0.000000 |\n"
     ]
    }
   ],
   "source": [
    "@cflearn.register_metric(\"my_metric_with_loader\", allow_duplicate=False)\n",
    "class MyMetricWithLoader(cflearn.MetricInterface):\n",
    "    @property\n",
    "    def is_positive(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def forward(self, logits, labels, loader) -> float:\n",
    "        print(f\"\\n>>> loader      : {loader}\")\n",
    "        print(f\">>> loader.data : {loader.data}\\n\")\n",
    "        return 0.0\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    metric_names=\"my_metric_with_loader\",\n",
    "    **common_kwargs,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
