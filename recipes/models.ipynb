{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Models Recipes\n",
    "\n",
    "In this page, we will show you how to customize your own models. In `carefree-learn`, it is fairly easy to define various kinds of models with three APIs: `register_ml_module` (for [ML models](#ML-Models)), `register_module` (for [Other Models](#Other-Models)) and `register_custom_module` (for [Complex Models](#Complex-Models)).\n",
    "\n",
    "> You might notice that if you run the blocks with `register_*` calls for more than once, `carefree-learn` will throw a warning which says \" '...' has already been registered \", and your changes will have no effect. This is intentional because normally we **DO NOT** want to register anything for more than once.\n",
    "> \n",
    "> However, if you are using some interactive developing tools (e.g. Jupyter Notebook), it is very common to modify the implementations for more than once. In this case, we can set `allow_duplicate=True` in the `register_*` functions to bypass this check. And of course, this should **NEVER** happen in production for safety!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "\n",
    "- [One-Stage Models](#One-Stage-Models)\n",
    "  - [ML Models](#ML-Models)\n",
    "    - [Configurations](#Configurations)\n",
    "  - [Other Models](#Other-Models)\n",
    "- [Complex Models](#Complex-Models)\n",
    "- [Appendix](#Appendix)\n",
    "  - [ML Encodings](#ML-Encodings)\n",
    "    - [Optimizations](#Optimizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You might also notice that:\n",
    "> - The class name defined below somehow matches the registered name. This is also not required, since `carefree-learn` only cares about the name that you pass to the `register_*` function, and will not check the actual class name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import cflearn\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Dict\n",
    "\n",
    "try:\n",
    "    from sklearn import metrics\n",
    "except:\n",
    "    metrics = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Stage Models\n",
    "\n",
    "We will first jump into the typical situation where we need to define one-stage models. The 'one-stage' here means that the training step only contains one optimizer step, so we can focus on how to define the forward pass of our models, and leave `carefree-learn` to handle other stuffs.\n",
    "\n",
    "> The contrary of 'one-stage' models will be the [Complex Models](#Complex-Models). A typical 'complex' model is the `GAN` models, which in general should perform a generator optimizing step **AND** a discriminator optimizing step in one **SINGLE** training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `carefree-learn`, Machine Learning Models will be slightly different to other models because:\n",
    "- We have integrated some common data-preprocessing methods into the ML pipeline (e.g. `one_hot` encoding, `embedding`).\n",
    "- There are some shared arguments that should be used by all ML models: input dimension, output dimension and number of history steps (this is used in timeseries tasks).\n",
    "\n",
    "Therefore, `carefree-learn` has:\n",
    "- Wrapped the registered `nn.Module` internally to make it suitable for ML pipeline.\n",
    "- Introduced three (optional) pre-defined arguments for all ML Models: `input_dim`, `output_dim` and `num_history`.\n",
    "  - We call it the 'dimension system' of `carefree-learn`.\n",
    "\n",
    "We will dive into these details in the following sections step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cflearn.register_ml_module(\"my_linear0\", allow_duplicate=False)\n",
    "class MyLinear0(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, net: Tensor) -> Tensor:\n",
    "        return self.net(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We can now integrate them into our ML pipeline with the `fit_ml` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    MyLinear0                                [-1, 5]                                  [-1, 2]                   12\n",
      "      Linear                                 [-1, 5]                                  [-1, 2]                   12\n",
      "========================================================================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-24_18-41-46-523792\\checkpoints\n",
      "| epoch  -1  [-1 / 1] [0.043s] | acc : 0.540000 | auc : 0.494766 | score : 0.517383 |\n"
     ]
    }
   ],
   "source": [
    "n       = 100\n",
    "in_dim  = 5\n",
    "out_dim = 2\n",
    "\n",
    "x = np.random.random([n, in_dim])\n",
    "y = np.random.randint(0, 2, [n, 1])\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    x,\n",
    "    y,\n",
    "    core_name=\"my_linear0\",\n",
    "    output_dim=out_dim,\n",
    "    is_classification=True,\n",
    "    # debug setting, indicating that we only train for one step\n",
    "    fixed_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `output_dim` passed to `fit_ml` will be passed into your model as well.\n",
    "- The `input_dim` is not provided, and `carefree-learn` will use `x.shape[1]` as `input_dim`.\n",
    "\n",
    "If the `input_dim` is specified, we will use it regardless of `x.shape[1]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    MyLinear1                                [-1, 5]                                  [-1, 2]                   22\n",
      "      Linear                                [-1, 10]                                  [-1, 2]                   22\n",
      "========================================================================================================================\n",
      "Total params: 22\n",
      "Trainable params: 22\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-24_18-41-46-625798\\checkpoints\n",
      "| epoch  -1  [-1 / 1] [0.015s] | acc : 0.550000 | auc : 0.513687 | score : 0.531843 |\n"
     ]
    }
   ],
   "source": [
    "@cflearn.register_ml_module(\"my_linear1\", allow_duplicate=False)\n",
    "class MyLinear1(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, net: Tensor) -> Tensor:\n",
    "        # duplicate the input\n",
    "        return self.net(torch.cat([net, net], dim=1))\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    x,\n",
    "    y,\n",
    "    core_name=\"my_linear1\",\n",
    "    # the input is duplicated, so we need to specify the `input_dim`\n",
    "    input_dim=5 * 2,\n",
    "    output_dim=out_dim,\n",
    "    is_classification=True,\n",
    "    # debug setting, indicating that we only train for one step\n",
    "    fixed_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that in the 'summary' panel shown above, the `MyLinear1` module is 'wrapped' by `MLModel` and `_`. This is what `carefree-learn` does internally to make your model compatible for the ML pipeline.\n",
    "\n",
    "Here's an example, with `one_hot` encoding and `embedding` considered, to show you why this kind of 'wrapping' is useful and powerful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cflearn import MERGED_KEY\n",
    "from cflearn import ONE_HOT_KEY\n",
    "from cflearn import EMBEDDING_KEY\n",
    "from cflearn import NUMERICAL_KEY\n",
    "\n",
    "@cflearn.register_ml_module(\"my_linear2\", allow_duplicate=False)\n",
    "class MyLinear2(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        print(\"> input_dim\", input_dim)\n",
    "        self.net = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    # notice that we use `batch` here, and the naming is important!\n",
    "    def forward(self, batch: Dict[str, Tensor]) -> Tensor:\n",
    "        merged = batch[MERGED_KEY]\n",
    "        one_hot = batch[ONE_HOT_KEY]\n",
    "        embedding = batch[EMBEDDING_KEY]\n",
    "        numerical = batch[NUMERICAL_KEY]\n",
    "        print()\n",
    "        print(\">>> merged\", merged.shape)\n",
    "        if one_hot is not None:\n",
    "            print(\">>> one_hot\", one_hot.shape)\n",
    "        if embedding is not None:\n",
    "            print(\">>> embedding\", embedding.shape)\n",
    "        if numerical is not None:\n",
    "            print(\">>> numerical\", numerical.shape)\n",
    "        print()\n",
    "        return self.net(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As the comment says, the naming of the `forward` argument, `batch`, is important! Because `carefree-learn` will then know that you require the full batch, instead of a single `Tensor`.\n",
    "\n",
    "The newly defined `my_linear1` model can be used as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> input_dim 5\n",
      "\n",
      ">>> merged torch.Size([1, 5])\n",
      ">>> numerical torch.Size([1, 5])\n",
      "\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    MyLinear2                                                                                                     \n",
      "      Linear                                 [-1, 5]                                  [-1, 2]                   12\n",
      "========================================================================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "\n",
      ">>> merged torch.Size([100, 5])\n",
      ">>> numerical torch.Size([100, 5])\n",
      "\n",
      "\n",
      ">>> merged torch.Size([100, 5])\n",
      ">>> numerical torch.Size([100, 5])\n",
      "\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-24_18-41-46-696792\\checkpoints\n",
      "\n",
      ">>> merged torch.Size([100, 5])\n",
      ">>> numerical torch.Size([100, 5])\n",
      "\n",
      "| epoch  -1  [-1 / 1] [0.026s] | acc : 0.460000 | auc : 0.434782 | score : 0.447391 |\n"
     ]
    }
   ],
   "source": [
    "m = cflearn.api.fit_ml(\n",
    "    x,\n",
    "    y,\n",
    "    core_name=\"my_linear2\",\n",
    "    output_dim=out_dim,\n",
    "    is_classification=True,\n",
    "    # debug setting, indicating that we only train for one step\n",
    "    fixed_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the powerful part is that, it can now utilize the encoding methods (`one_hot` / `embedding`) provided by `carefree-learn`:\n",
    "\n",
    "> We will use some encoding settings in a few following blocks. Please refer to the [Appendix](#ML-Encodings) section for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> input_dim 26\n",
      "\n",
      ">>> merged torch.Size([1, 26])\n",
      ">>> one_hot torch.Size([1, 13])\n",
      ">>> embedding torch.Size([1, 8])\n",
      ">>> numerical torch.Size([1, 5])\n",
      "\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  Encoder                                    [-1, 8]                      [[-1, 13], [-1, 8]]                   56\n",
      "    ModuleDict-1                                                                                                  \n",
      "      OneHot                                    [-1]                                 [-1, 13]                    0\n",
      "        Lambda                                  [-1]                                 [-1, 13]                    0\n",
      "    ModuleDict-0                                                                                                  \n",
      "      Embedding                              [-1, 2]                               [-1, 2, 4]                   56\n",
      "        Lambda                               [-1, 2]                               [-1, 2, 4]                    0\n",
      "    Dropout                                  [-1, 8]                                  [-1, 8]                    0\n",
      "  _                                                                                                               \n",
      "    MyLinear2                                                                                                     \n",
      "      Linear                                [-1, 26]                                  [-1, 2]                   54\n",
      "========================================================================================================================\n",
      "Total params: 110\n",
      "Trainable params: 110\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "\n",
      ">>> merged torch.Size([100, 26])\n",
      ">>> one_hot torch.Size([100, 13])\n",
      ">>> embedding torch.Size([100, 8])\n",
      ">>> numerical torch.Size([100, 5])\n",
      "\n",
      "\n",
      ">>> merged torch.Size([100, 26])\n",
      ">>> one_hot torch.Size([100, 13])\n",
      ">>> embedding torch.Size([100, 8])\n",
      ">>> numerical torch.Size([100, 5])\n",
      "\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-24_18-41-46-779788\\checkpoints\n",
      "\n",
      ">>> merged torch.Size([100, 26])\n",
      ">>> one_hot torch.Size([100, 13])\n",
      ">>> embedding torch.Size([100, 8])\n",
      ">>> numerical torch.Size([100, 5])\n",
      "\n",
      "| epoch  -1  [-1 / 1] [0.021s] | acc : 0.510000 | auc : 0.475560 | score : 0.492780 |\n"
     ]
    }
   ],
   "source": [
    "n                 = 100\n",
    "in_dim            = 5\n",
    "one_hot_dim       = 13\n",
    "embedding_dim     = 7\n",
    "out_dim           = 2\n",
    "# some encoding settings. Please refer to the `ML Encodings` section in the `Appendix` section for more details.\n",
    "one_hot_setting   = dict(dim=one_hot_dim, methods=\"one_hot\")\n",
    "embedding_setting = dict(dim=embedding_dim, methods=\"embedding\")\n",
    "encoding_settings = {\n",
    "    # one hot columns   : [6]\n",
    "    6: one_hot_setting,\n",
    "    # embedding columns : [5, 7] \n",
    "    5: embedding_setting,\n",
    "    7: embedding_setting,\n",
    "}\n",
    "\n",
    "x = np.hstack([\n",
    "    np.random.random([n, in_dim]),\n",
    "    np.random.randint(0, embedding_dim, [n, 1]),\n",
    "    np.random.randint(0, one_hot_dim, [n, 1]),\n",
    "    np.random.randint(0, embedding_dim, [n, 1]),\n",
    "])\n",
    "y = np.random.randint(0, 2, [n, 1])\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    x,\n",
    "    y,\n",
    "    core_name=\"my_linear2\",\n",
    "    output_dim=out_dim,\n",
    "    is_classification=True,\n",
    "    # encoding settings\n",
    "    encoding_settings=encoding_settings,\n",
    "    # debug setting, indicating that we only train for one step\n",
    "    fixed_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We don't need to specify the `input_dim`. In this case, `carefree-learn` will use `merged_dim` as `input_dim`.\n",
    "- `merged_dim` = `one_hot_dim` + `embedding_dim` + `numerical_dim`, because `carefree-learn` will simply concat every kind of inputs together to create the `merged` input.\n",
    "- In the 'summary' panel, we can find that the `Embedding` module output `4` dimension `Tensor` with `56` trainable params. That's because we have `2` columns for embedding, each has `7` different values, so `56 = 2 * 7 * 4`.\n",
    "\n",
    "Although it is already very powerful to have access to every part of the inputs, it is still pretty hard to utilize them, because currently we can only get the `merged_dim` in our `__init__` method. `carefree-learn` therefore provides a `dimensions` argument that gives you all you want.\n",
    "\n",
    "For example, let's implement the famous [Wide & Deep](https://arxiv.org/pdf/1606.07792.pdf)-like model, which feeds the `one_hot` part to a `Linear` model, and feeds the `numerical` and `embedding` part to a `MLP` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cflearn.register_ml_module(\"my_wide_and_deep\", allow_duplicate=True)\n",
    "class MyWideAndDeep(nn.Module):\n",
    "    # notice that we use `dimensions` here, and the naming is important!\n",
    "    def __init__(self, dimensions, output_dim):\n",
    "        super().__init__()\n",
    "        print(\">\", dimensions)\n",
    "        self.wide = nn.Linear(dimensions.one_hot_dim, output_dim)\n",
    "        self.deep = nn.Sequential(\n",
    "            nn.Linear(dimensions.embedding_dim + dimensions.numerical_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch: Dict[str, Tensor]) -> Tensor:\n",
    "        one_hot = batch[ONE_HOT_KEY]\n",
    "        embedding = batch[EMBEDDING_KEY]\n",
    "        numerical = batch[NUMERICAL_KEY]\n",
    "        wide_output = self.wide(one_hot)\n",
    "        deep_output = self.deep(torch.cat([embedding, numerical], dim=1))\n",
    "        return wide_output + deep_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As the comment says, the naming of the `__init__` argument, `dimensions`, is important! Because `carefree-learn` will then know that you require the full `dimensions` information, instead of a single `input_dim`.\n",
    "\n",
    "We can run it and see if it works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Dimensions(\n",
      "    merged_dim    = 26\n",
      "    one_hot_dim   = 13\n",
      "    embedding_dim = 8\n",
      "    numerical_dim = 5\n",
      ")\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  Encoder                                    [-1, 8]                      [[-1, 13], [-1, 8]]                   56\n",
      "    ModuleDict-1                                                                                                  \n",
      "      OneHot                                    [-1]                                 [-1, 13]                    0\n",
      "        Lambda                                  [-1]                                 [-1, 13]                    0\n",
      "    ModuleDict-0                                                                                                  \n",
      "      Embedding                              [-1, 2]                               [-1, 2, 4]                   56\n",
      "        Lambda                               [-1, 2]                               [-1, 2, 4]                    0\n",
      "    Dropout                                  [-1, 8]                                  [-1, 8]                    0\n",
      "  _                                                                                                               \n",
      "    MyWideAndDeep                                                                                                 \n",
      "      Linear                                [-1, 13]                                  [-1, 2]                   28\n",
      "      Sequential                            [-1, 13]                                  [-1, 2]                2,050\n",
      "        Linear-0                            [-1, 13]                                [-1, 128]                1,792\n",
      "        ReLU                               [-1, 128]                                [-1, 128]                    0\n",
      "        Linear-1                           [-1, 128]                                  [-1, 2]                  258\n",
      "========================================================================================================================\n",
      "Total params: 2,134\n",
      "Trainable params: 2,134\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.01\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-24_18-41-46-866792\\checkpoints\n",
      "| epoch  -1  [-1 / 1] [0.101s] | acc : 0.470000 | auc : 0.524839 | score : 0.497419 |\n"
     ]
    }
   ],
   "source": [
    "m = cflearn.api.fit_ml(\n",
    "    x,\n",
    "    y,\n",
    "    core_name=\"my_wide_and_deep\",\n",
    "    output_dim=out_dim,\n",
    "    is_classification=True,\n",
    "    # encoding settings\n",
    "    encoding_settings=encoding_settings,\n",
    "    # debug setting, indicating that we only train for one step\n",
    "    fixed_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bravo! Everything works like a charm! ðŸ¥³"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Configurations\n",
    "\n",
    "So far we've introduced the 'dimension system' of the ML Models in `carefree-learn`, but you might want to know how to use custom hyper-parameters in your own models. For example, to specify `use_bias` in `my_linear`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cflearn.register_ml_module(\"my_linear3\", allow_duplicate=False)\n",
    "class MyLinear3(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, *, use_bias: bool):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(input_dim, output_dim, bias=use_bias)\n",
    "    \n",
    "    def forward(self, net):\n",
    "        return self.net(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use `my_linear3` directly without specifying configurations, `carefree-learn` will throw an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__() missing 1 required keyword-only argument: 'use_bias'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    m = cflearn.api.fit_ml(\n",
    "        x,\n",
    "        y,\n",
    "        core_name=\"my_linear3\",\n",
    "        output_dim=out_dim,\n",
    "        is_classification=True,\n",
    "        # debug setting, indicating that we only train for one step\n",
    "        fixed_steps=1,\n",
    "    )\n",
    "except TypeError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the error indicates, we are missing `use_bias` to initialize the `MyLinear3` module. To fix it, we can add a `core_config` part to the `fit_ml` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    MyLinear3                                [-1, 8]                                  [-1, 2]                   16\n",
      "      Linear                                 [-1, 8]                                  [-1, 2]                   16\n",
      "========================================================================================================================\n",
      "Total params: 16\n",
      "Trainable params: 16\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-24_18-41-47-051789\\checkpoints\n",
      "| epoch  -1  [-1 / 1] [0.013s] | acc : 0.520000 | auc : 0.443108 | score : 0.481554 |\n"
     ]
    }
   ],
   "source": [
    "m = cflearn.api.fit_ml(\n",
    "    x,\n",
    "    y,\n",
    "    core_name=\"my_linear3\",\n",
    "    # Add This!\n",
    "    core_config=dict(use_bias=False),\n",
    "    output_dim=out_dim,\n",
    "    is_classification=True,\n",
    "    # debug setting, indicating that we only train for one step\n",
    "    fixed_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the trainable parameters of `Linear` is only `16`, which means the `bias` has indeed been set to `False`.\n",
    "\n",
    "> We put `use_bias` after a `*` to make it a keyword-only argument. It is not forced to do so, but in general it's recommended because:\n",
    "> - It will make your module easier to understand when it is used by others.\n",
    "> - It can separate the `carefree-learn`'s 'dimension system' from your own custom configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models\n",
    "\n",
    "Besides ML Models, customizing other models (e.g. CV models) with `register_module` API is almost the same as writing custom `nn.Module`. For example, let's build a simple image classification model from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cflearn.register_module(\"my_image_classifier\", allow_duplicate=True)\n",
    "class MyImageClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, img_size, output_dim):\n",
    "        super().__init__()\n",
    "        flat_dim = in_channels * img_size ** 2\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(flat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, net):\n",
    "        # flatten the input first\n",
    "        net = net.view(net.shape[0], -1)\n",
    "        return self.net(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike ML Models, for any other models, `carefree-learn` will not 'inject' any pre-defined arguments to your `nn.Module`, so it will be safe & clean! ðŸ˜‰\n",
    "\n",
    "> In fact, even for ML Models, you can ignore the 'dimension system' completely! Just avoid using names like `in_dim` / `input_dim` / `out_dim` / `output_dim` and everything will be fine.\n",
    "\n",
    "We can play around with the `my_image_classifier` model on the famous `MNIST` dataset with `fit_cv` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "_                                                                                                                 \n",
      "  MyImageClassifier                  [-1, 1, 28, 28]                                 [-1, 10]              101,770\n",
      "    Sequential                             [-1, 784]                                 [-1, 10]              101,770\n",
      "      Linear-0                             [-1, 784]                                [-1, 128]              100,480\n",
      "      ReLU                                 [-1, 128]                                [-1, 128]                    0\n",
      "      Linear-1                             [-1, 128]                                 [-1, 10]                1,290\n",
      "========================================================================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.39\n",
      "Estimated Total Size (MB): 0.39\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-24_18-41-47-187791\\checkpoints\n",
      "| epoch  -1  [   -1 / 30000] [0.025s] | acc : 0.500000 | score : 0.500000 |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cflearn.api.cv.pipeline.CarefreePipeline at 0x21a6fcc2940>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get MNIST data with `cflearn`'s predefined API\n",
    "data = cflearn.cv.MNISTData(batch_size=2, transform=\"to_tensor\")\n",
    "# use `fit_cv` API for training\n",
    "cflearn.api.fit_cv(\n",
    "    # This first argument passed to `fit_cv` is complicated and hard to explain briefly\n",
    "    # So we will cover its details in another article (the `Data Recipes`)\n",
    "    data,\n",
    "    # this is the name of your model\n",
    "    model_name=\"my_image_classifier\",\n",
    "    # these (and only these) settings will go into your model's __init__ method\n",
    "    model_config={\"in_channels\": 1, \"img_size\": 28, \"output_dim\": 10},\n",
    "    # these are some training settings\n",
    "    loss_name=\"cross_entropy\",\n",
    "    metric_names=\"acc\",\n",
    "    # debug setting, indicating that we only use a small portion of data to do validation\n",
    "    valid_portion=1.0e-5,\n",
    "    # debug setting, indicating that we only train for one step\n",
    "    fixed_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that the naming is slightly different from ML Models:\n",
    "- `core_name` -> `model_name`\n",
    "- `core_config` -> `model_config`\n",
    "\n",
    "This is because ML Models will 'wrap' customized models under the `MLModel`, which means they serve as the `core` of `MLModel`. That's why we use `core_name` & `core_config`. But for other situations, the customized models will be left as-is, so we use `model_name` & `model_config`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Encodings\n",
    "\n",
    "What makes Machine Learning tasks different from other tasks is that some of the data-preprocessing methods are 'trainable' (e.g. embedding). In this case, we need to integrate these methods into our models rather than simply put them in a separate place.\n",
    "\n",
    "It is OK to implement these methods in our custom models every time when we need them, but that will cause a **LOT** of boilerplate codes. `carefree-learn` therefore extracted them into an `Encoder` module, and exposed its settings in the APIs for you to utilize it easily.\n",
    "\n",
    "The definitions related to the `Encoder` are pretty simple:\n",
    "\n",
    "```python\n",
    "class EncodingSettings(NamedTuple):\n",
    "    dim: int\n",
    "    methods: Union[str, List[str]] = \"embedding\"\n",
    "    method_configs: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        settings: Dict[int, EncodingSettings],\n",
    "        *,\n",
    "        # there are a few more kwargs here, which will be covered in the `Optimizations` section\n",
    "        ...\n",
    "    ):\n",
    "        ...\n",
    "```\n",
    "\n",
    "The `settings` of the `Encoder` is what we should mainly pay attention to: it's a mapping that maps the column index to its corresponding `EncodingSettings`.\n",
    "\n",
    "For example, if:\n",
    "- Our input features, `x`, has 10 columns, then the column index should be [0, 1, 2, ..., 9]\n",
    "- The `0`th, `5`th & `7`th column are categorical columns, and they have `5`, `7` & `11` unique values respectively.\n",
    "- We want to apply `one_hot` to the `0`th & `5`th column.\n",
    "- We want to apply `embedding` to the `5`th & `7`th column.\n",
    "\n",
    "Then the corresponding setup should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embeddings): ModuleDict(\n",
       "    (-1): Embedding(\n",
       "      (core): Lambda(embedding: 12 -> 4)\n",
       "    )\n",
       "  )\n",
       "  (one_hot_encoders): ModuleDict(\n",
       "    (5): OneHot(\n",
       "      (core): Lambda(one_hot_7)\n",
       "    )\n",
       "    (7): OneHot(\n",
       "      (core): Lambda(one_hot_11)\n",
       "    )\n",
       "  )\n",
       "  (embedding_dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cflearn.models.ml.encoders import Encoder\n",
    "from cflearn.models.ml.encoders import EncodingSettings\n",
    "\n",
    "Encoder(\n",
    "    {\n",
    "        0: EncodingSettings(5),\n",
    "        5: EncodingSettings(7, [\"one_hot\", \"embedding\"]),\n",
    "        7: EncodingSettings(11, \"one_hot\"),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that the `one_hot_encoders` part is pretty straight forward: we initialized a `ModuleDict` which mapped the column index into its corresponding `OneHot` encoder, and the dimension matches exactly to the number of unique values. However, the `embeddings` part is a little weird: we only initialized one `Embedding` module, and the dimension is `12`, which is exactly `5+7` - the sum of the number of unique values.\n",
    "\n",
    "This is due to a special mechanism in `carefree-learn` - the `fast_embedding` mechanism. We will cover the details in the [next section](#Optimizations), for now let's just see how to disable this mechanism and make our `Encoder` looks 'normal':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embeddings): ModuleDict(\n",
       "    (0): Embedding(\n",
       "      (core): Lambda(embedding: 5 -> 4)\n",
       "    )\n",
       "    (5): Embedding(\n",
       "      (core): Lambda(embedding: 7 -> 4)\n",
       "    )\n",
       "  )\n",
       "  (one_hot_encoders): ModuleDict(\n",
       "    (5): OneHot(\n",
       "      (core): Lambda(one_hot_7)\n",
       "    )\n",
       "    (7): OneHot(\n",
       "      (core): Lambda(one_hot_11)\n",
       "    )\n",
       "  )\n",
       "  (embedding_dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Encoder(\n",
    "    {\n",
    "        0: EncodingSettings(5),\n",
    "        5: EncodingSettings(7, [\"one_hot\", \"embedding\"]),\n",
    "        7: EncodingSettings(11, \"one_hot\"),\n",
    "    },\n",
    "    config={\n",
    "        \"use_fast_embedding\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now the `embedding` part looks exactly the same as the `one_hot_encoders` part: we initialized a `ModuleDict` which mapped the column index into its corresponding `Embedding` encoder, and the dimension matches exactly to the number of unique values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
