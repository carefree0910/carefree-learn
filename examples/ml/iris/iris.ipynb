{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris\n",
    "\n",
    "Here are some of the information provided by the official website:\n",
    "\n",
    "```text\n",
    "This is perhaps the best known database to be found in the pattern recognition literature.\n",
    "The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.\n",
    "Predicted attribute: class of iris plant.\n",
    "```\n",
    "\n",
    "And here's the pandas-view of the raw data:\n",
    "\n",
    "```text\n",
    "      f0   f1   f2   f3           label\n",
    "0    5.1  3.5  1.4  0.2     Iris-setosa\n",
    "1    4.9  3.0  1.4  0.2     Iris-setosa\n",
    "2    4.7  3.2  1.3  0.2     Iris-setosa\n",
    "3    4.6  3.1  1.5  0.2     Iris-setosa\n",
    "4    5.0  3.6  1.4  0.2     Iris-setosa\n",
    "..   ...  ...  ...  ...             ...\n",
    "145  6.7  3.0  5.2  2.3  Iris-virginica\n",
    "146  6.3  2.5  5.0  1.9  Iris-virginica\n",
    "147  6.5  3.0  5.2  2.0  Iris-virginica\n",
    "148  6.2  3.4  5.4  2.3  Iris-virginica\n",
    "149  5.9  3.0  5.1  1.8  Iris-virginica\n",
    "\n",
    "[150 rows x 5 columns]\n",
    "```\n",
    "\n",
    "> We didn't use pandas in our code, but it is convenient to visualize some data with it though ðŸ¤£\n",
    ">\n",
    "> You can download the raw data (`iris.data`) with [this link](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparations\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import cflearn\n",
    "import numpy as np\n",
    "from cflearn.misc.toolkit import seed_everything\n",
    "\n",
    "seed_everything(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Usages\n",
    "\n",
    "Traditionally, we need to process the raw data before we feed them into our machine learning models (e.g. encode the label column, which is a string column, into an ordinal column). In carefree-learn, however, we can train neural networks directly on files without worrying about the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [warning] neither `label_names` nor `label_indices` is provided, `[-1]` will be used\n",
      "================================================================================================================================\n",
      "                                    Internal Default Configurations Used by `carefree-learn`                                    \n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                   train_samples   |   125\n",
      "                                                   valid_samples   |   25\n",
      "                                               max_snapshot_file   |   25\n",
      "                                                encoder_settings   |   {}\n",
      "                                                       workspace   |   _logs\\2023-03-19_21-40-56-199595\n",
      "                                   model_config.encoder_settings   |   {}\n",
      "                                                 index_mapping.0   |   0\n",
      "                                                 index_mapping.1   |   1\n",
      "                                                 index_mapping.2   |   2\n",
      "                                                 index_mapping.3   |   3\n",
      "                                                   monitor_names   |   ['mean_std', 'plateau']\n",
      "                                            additional_callbacks   |   ['_log_metrics_msg']\n",
      "                                         log_metrics_msg_verbose   |   True\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "================================================================================================================================\n",
      "                                                    External Configurations                                                     \n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                      model_name   |   fcnn\n",
      "                                  state_config.max_snapshot_file   |   25\n",
      "                              state_config.num_step_per_snapshot   |   1\n",
      "                                state_config.snapshot_start_step   |   24\n",
      "                                                    metric_names   |   ['acc', 'auc']\n",
      "                                                  callback_names   |   ['_log_metrics_msg']\n",
      "                       callback_configs._log_metrics_msg.verbose   |   True\n",
      "                                          tqdm_settings.use_tqdm   |   False\n",
      "                                     tqdm_settings.use_step_tqdm   |   False\n",
      "                            tqdm_settings.use_tqdm_in_validation   |   False\n",
      "                                    tqdm_settings.in_distributed   |   False\n",
      "                                     tqdm_settings.tqdm_position   |   0\n",
      "                                         tqdm_settings.tqdm_desc   |   epoch\n",
      "                                                       loss_name   |   focal\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "FCNN                                                                                                              \n",
      "  Sequential                                 [-1, 4]                                  [-1, 3]                1,315\n",
      "    Mapping-0                                [-1, 4]                                 [-1, 32]                  160\n",
      "      Linear                                 [-1, 4]                                 [-1, 32]                  160\n",
      "      ReLU                                  [-1, 32]                                 [-1, 32]                    0\n",
      "    Mapping-1                               [-1, 32]                                 [-1, 32]                1,056\n",
      "      Linear                                [-1, 32]                                 [-1, 32]                1,056\n",
      "      ReLU                                  [-1, 32]                                 [-1, 32]                    0\n",
      "    Linear                                  [-1, 32]                                  [-1, 3]                   99\n",
      "========================================================================================================================\n",
      "Total params: 1,315\n",
      "Trainable params: 1,315\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.01\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      ">  [ info ] restoring from _logs\\2023-03-19_21-40-56-199595\\checkpoints\\model_82.pt\n",
      "| epoch  -1  [-1 / 1] [0.350s] | acc : 0.920000 | auc : 1.000000 | score : 0.960000 |\n"
     ]
    }
   ],
   "source": [
    "processor_config = cflearn.MLBundledProcessorConfig(has_header=False, num_split=25)\n",
    "data = cflearn.MLData.init(processor_config=processor_config).fit(\"iris.data\")\n",
    "config = cflearn.MLConfig(\n",
    "    model_name=\"fcnn\",\n",
    "    model_config=dict(input_dim=data.num_features, output_dim=data.num_labels),\n",
    "    loss_name=\"focal\",\n",
    "    metric_names=[\"acc\", \"auc\"],\n",
    ")\n",
    "m = cflearn.api.fit_ml(data, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going under the hood is that carefree-learn will try to parse the `iris.data` automatically, split the data into training set and validation set, with which we'll train a fully connected neural network (fcnn).\n",
    "\n",
    "We can further inspect the processed data if we want to know how carefree-learn actually parsed the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> mean [-0.02245645 -0.00092561 -0.01379941 -0.01192022]\n",
      "> std  [0.99158337 1.00485133 0.9964612  0.98410995]\n"
     ]
    }
   ],
   "source": [
    "data = m.data\n",
    "x_train = data.train_dataset.x\n",
    "print(\"> mean\", x_train.mean(0))\n",
    "print(\"> std \", x_train.std(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that the raw data is carefully normalized into numerical data that neural networks can accept. What's more, by saying *normalized*, it means that the input features will be automatically normalized to `mean=0.0` and `std=1.0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> mean [-4.42608912e-16 -7.05546732e-16  1.93918955e-16  6.80936788e-16]\n",
      "> std  [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "data = m.data\n",
    "x_train = data.train_dataset.x\n",
    "x_valid = data.valid_dataset.x\n",
    "stacked = np.vstack([x_train, x_valid])\n",
    "print(\"> mean\", stacked.mean(0))\n",
    "print(\"> std \", stacked.std(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The results shown above means we first normalized the data before we actually split it into train & validation set.\n",
    "\n",
    "After training on files, carefree-learn can predict & evaluate on files directly as well. We'll handle the data parsing and normalization for you automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================================================================\n",
      "|        metrics         |                       acc                        |                       auc                        |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|           m            |    0.853333    |    0.000000    |    0.853333    |    0.977733    |    0.000000    |    0.977733    |\n",
      "================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': {'m': Statistics(sign=1.0, mean=0.8533333333333334, std=0.0, score=0.8533333333333334)},\n",
       " 'auc': {'m': Statistics(sign=1.0, mean=0.9777333333333332, std=0.0, score=0.9777333333333332)}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = data.build_loader(\"iris.data\")\n",
    "predictions = m.predict(loader)\n",
    "# evaluations could be achieved easily with cflearn.api.evaluate\n",
    "cflearn.api.evaluate(loader, dict(m=m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking\n",
    "\n",
    "As we know, neural networks are trained with **_stochastic_** gradient descent (and its variants), which will introduce some randomness to the final result, even if we are training on the same dataset. In this case, we need to repeat the same task several times in order to obtain the bias & variance of our neural networks.\n",
    "\n",
    "Fortunately, carefree-learn introduced `repeat_ml` API, which can achieve this goal easily with only a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================================================================\n",
      "|        metrics         |                       acc                        |                       auc                        |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|          fcnn          |    0.842222    |    0.008314    |    0.833907    |    0.975111    |    0.013052    |    0.962058    |\n",
      "================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': {'fcnn': Statistics(sign=1.0, mean=0.8422222222222223, std=0.008314794192830995, score=0.8339074280293913)},\n",
       " 'auc': {'fcnn': Statistics(sign=1.0, mean=0.9751111111111112, std=0.013052864970979294, score=0.9620582461401319)}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With num_repeat=3 specified, we'll train 3 models on `iris.data`.\n",
    "results = cflearn.api.repeat_ml(data, m.config, num_repeat=3)\n",
    "pipelines = cflearn.api.load_pipelines(results)\n",
    "cflearn.api.evaluate(loader, pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare the performances across different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:17<00:00,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================================================================\n",
      "|        metrics         |                       acc                        |                       auc                        |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|          fcnn          | -- 0.860000 -- | -- 0.056829 -- | -- 0.803170 -- | -- 0.966088 -- | -- 0.016351 -- | -- 0.949737 -- |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|         linear         |    0.562222    |    0.388132    |    0.174089    |    0.675977    |    0.363547    |    0.312429    |\n",
      "================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': {'fcnn': Statistics(sign=1.0, mean=0.86, std=0.056829830455752954, score=0.803170169544247),\n",
       "  'linear': Statistics(sign=1.0, mean=0.5622222222222222, std=0.38813259793561133, score=0.17408962428661084)},\n",
       " 'auc': {'fcnn': Statistics(sign=1.0, mean=0.9660888888888888, std=0.016351086345908615, score=0.9497378025429801),\n",
       "  'linear': Statistics(sign=1.0, mean=0.6759777777777778, std=0.36354788500465524, score=0.31242989277312255)}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With models=[\"linear\", \"fcnn\"], we'll train both linear models and fcnn models.\n",
    "models = [\"linear\", \"fcnn\"]\n",
    "results = cflearn.api.repeat_ml(data, m.config, models=models, num_repeat=3)\n",
    "pipelines = cflearn.api.load_pipelines(results)\n",
    "cflearn.api.evaluate(loader, pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth mentioning that carefree-learn supports distributed training, which means when we need to perform large scale benchmarking (e.g. train 100 models), we could accelerate the process through multiprocessing:\n",
    "\n",
    "> In `carefree-learn`, Distributed Training in Machine Learning tasks sometimes doesn't mean training your model on multiple GPUs or multiple machines. Instead, it may mean training multiple models at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:34<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "# With num_jobs=2, we will launch 2 processes to run the tasks in a distributed way.\n",
    "results = cflearn.api.repeat_ml(data, m.config, models=models, num_repeat=10, num_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On iris dataset, however, launching distributed training may actually hurt the speed because iris dataset only contains 150 samples, so the relative overhead brought by distributed training might be too large.\n",
    "\n",
    "### Advanced Benchmarking\n",
    "\n",
    "But this is not enough, because we want to know whether other models (e.g. scikit-learn models) could achieve a better performance than carefree-learn models. In this case, we can perform an advanced benchmarking with the `Experiment` helper class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\GitHub\\\\carefree-learn-new\\\\examples\\\\ml\\\\iris\\\\_experiment\\\\random_forest\\\\0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment = cflearn.dist.ml.Experiment()\n",
    "data_folder = experiment.dump_data(data)\n",
    "\n",
    "# Add carefree-learn tasks\n",
    "experiment.add_task(model=\"fcnn\", config=config, data_folder=data_folder)\n",
    "experiment.add_task(model=\"linear\", config=config, data_folder=data_folder)\n",
    "# Add scikit-learn tasks\n",
    "run_command = f\"python run_sklearn.py\"\n",
    "common_kwargs = {\"run_command\": run_command, \"data_folder\": data_folder}\n",
    "experiment.add_task(model=\"decision_tree\", **common_kwargs)\n",
    "experiment.add_task(model=\"random_forest\", **common_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we specified `run_command=\"python run_sklearn.py\"` for scikit-learn tasks, which means `Experiment` will try to execute this command in the current working directory for training scikit-learn models. The good news is that we do not need to speciy any command line arguments, because `Experiment` will handle those for us.\n",
    "\n",
    "Here is basically what a `run_sklearn.py` should look like ([source code](run_sklearn.py)):\n",
    "\n",
    "```python\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from cflearn.constants import INPUT_KEY\n",
    "from cflearn.constants import LABEL_KEY\n",
    "from cflearn.dist.ml.runs._utils import get_info\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    info = get_info()\n",
    "    meta = info.meta\n",
    "    # data\n",
    "    data = info.data\n",
    "    assert data is not None\n",
    "    data.prepare(None)\n",
    "    loader = data.initialize()[0]\n",
    "    dataset = loader.get_full_batch()\n",
    "    x, y = dataset[INPUT_KEY], dataset[LABEL_KEY]\n",
    "    assert isinstance(x, np.ndarray)\n",
    "    assert isinstance(y, np.ndarray)\n",
    "    # model\n",
    "    model = meta[\"model\"]\n",
    "    if model == \"decision_tree\":\n",
    "        base = DecisionTreeClassifier\n",
    "    elif model == \"random_forest\":\n",
    "        base = RandomForestClassifier\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    sk_model = base()\n",
    "    # train & save\n",
    "    sk_model.fit(x, y.ravel())\n",
    "    with open(os.path.join(info.workplace, \"sk_model.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(sk_model, f)\n",
    "\n",
    "```\n",
    "\n",
    "With `run_sklearn.py` defined, we could run those tasks with one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:10<00:00,  2.73s/it]\n"
     ]
    }
   ],
   "source": [
    "results = experiment.run_tasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finished running, we should be able to see the following file structure in the current working directory:\n",
    "\n",
    "```text\n",
    "|--- _experiment\n",
    "   |--- __data__\n",
    "      |-- npd\n",
    "      |-- id.txt\n",
    "      |-- info.json\n",
    "   |--- fcnn/0\n",
    "      |-- __meta__.json\n",
    "      |-- __dl_config__\n",
    "      |-- pipeline\n",
    "   |--- linear/0\n",
    "      |-- ...\n",
    "   |--- decision_tree/0\n",
    "      |-- __meta__.json\n",
    "      |-- sk_model.pkl\n",
    "   |--- random_forest/0\n",
    "      |-- ...\n",
    "```\n",
    "\n",
    "As we expected, `carefree-learn` pipeline are saved into the `pipeline` folder, while scikit-learn models are saved into `sk_model.pkl` files. Since these models are not yet loaded, we should manually load them into our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = cflearn.api.load_pipelines(results)\n",
    "for workspace, workspace_key in zip(results.workspaces, results.workspace_keys):\n",
    "    model = workspace_key[0]\n",
    "    if model in [\"decision_tree\", \"random_forest\"]:\n",
    "        model_file = os.path.join(workspace, \"sk_model.pkl\")\n",
    "        with open(model_file, \"rb\") as f:\n",
    "            predictor = cflearn.SKLearnClassifier(pickle.load(f))\n",
    "            pipelines[model] = cflearn.GeneralEvaluationPipeline(config, predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After which we can finally perform benchmarking on these models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================================================================\n",
      "|        metrics         |                       acc                        |                       auc                        |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|     decision_tree      | -- 1.000000 -- | -- 0.000000 -- | -- 1.000000 -- | -- 1.000000 -- | -- 0.000000 -- | -- 1.000000 -- |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|          fcnn          |    0.900000    | -- 0.000000 -- |    0.900000    |    0.977000    | -- 0.000000 -- |    0.977000    |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|         linear         |    0.886666    | -- 0.000000 -- |    0.886666    |    0.952466    | -- 0.000000 -- |    0.952466    |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|     random_forest      | -- 1.000000 -- | -- 0.000000 -- | -- 1.000000 -- | -- 1.000000 -- | -- 0.000000 -- | -- 1.000000 -- |\n",
      "================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\cflearn_new\\lib\\site-packages\\sklearn\\tree\\_classes.py:965: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(proba)\n",
      "D:\\anaconda3\\envs\\cflearn_new\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:910: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(proba)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': {'decision_tree': Statistics(sign=1.0, mean=1.0, std=0.0, score=1.0),\n",
       "  'fcnn': Statistics(sign=1.0, mean=0.9, std=0.0, score=0.9),\n",
       "  'linear': Statistics(sign=1.0, mean=0.8866666666666667, std=0.0, score=0.8866666666666667),\n",
       "  'random_forest': Statistics(sign=1.0, mean=1.0, std=0.0, score=1.0)},\n",
       " 'auc': {'decision_tree': Statistics(sign=1.0, mean=1.0, std=0.0, score=1.0),\n",
       "  'fcnn': Statistics(sign=1.0, mean=0.977, std=0.0, score=0.977),\n",
       "  'linear': Statistics(sign=1.0, mean=0.9524666666666667, std=0.0, score=0.9524666666666667),\n",
       "  'random_forest': Statistics(sign=1.0, mean=1.0, std=0.0, score=1.0)}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cflearn.api.evaluate(loader, pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Contained in this notebook is just a subset of the features that `carefree-learn` offers, but we've already walked through many basic & common steps we'll encounter in real life machine learning tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "dfcf4eb9f42f81729729aed89eabeac668d2d9675ff19e21733d40eea83e51c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
